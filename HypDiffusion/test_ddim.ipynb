{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhf/anaconda3/envs/anydoor/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-16 12:01:39,027] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import PIL\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '/data2/mhf/DXL/Lingxiao/Codes/HypDiffusion')\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm, trange\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from itertools import islice\n",
    "from einops import rearrange, repeat\n",
    "from torch import autocast\n",
    "from contextlib import nullcontext\n",
    "from pytorch_lightning import seed_everything\n",
    "import cv2\n",
    "import time\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "from ldm.models.diffusion.ddim_org import DDIMSampler\n",
    "from ldm.models.diffusion.dpm_solver_org import DPMSolverSampler\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neccessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils.pmath import *\n",
    "# we utilize geoopt package for hyperbolic calculation\n",
    "import geoopt.manifolds.stereographic.math as gmath\n",
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "\n",
    "def load_model_from_config(config, ckpt, verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_img(path, size=[256, 256]):\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    w, h = image.shape[:2]\n",
    "    # print(f\"loaded input image of size ({w}, {h}) from {path}\")\n",
    "    # resize to integer multiple of 32\n",
    "    # w, h = map(lambda x: x - x % 32, (w, h))\n",
    "    w, h = size\n",
    "    image = cv2.resize(image, (w, h), interpolation=cv2.INTER_LANCZOS4)\n",
    "    image = np.array(image).astype(np.uint8)\n",
    "\n",
    "    image = (image / 127.5 - 1.0).astype(np.float32)\n",
    "    image = image[None].transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "def get_unconditional_embedding(model, scale, n_samples, device, prompts):\n",
    "    # return the learned unconditioning\n",
    "    if scale != 1.0:\n",
    "        _, _, _, _, uc = model.get_learned_conditioning(\n",
    "            n_samples * [torch.zeros((1, 3, 224, 224)).to(device)])\n",
    "    else:\n",
    "        _, _, _, _, uc = model.get_learned_conditioning(prompts)\n",
    "\n",
    "    return uc\n",
    "\n",
    "def feature_fusion(model, prompt_1, prompt_2, alpha):\n",
    "    # fuse the feature with different attribute levels in hyperbolic space\n",
    "    '''\n",
    "    inputs:\n",
    "    prompt_1: the first image\n",
    "    prompt_2: the second image\n",
    "    alpha: the fusion ratio between the two prompts, value: [0, 6.2126]\n",
    "    \n",
    "    outputs:\n",
    "    fused_hyp_code: the fused latent code in hyperbolic space\n",
    "    '''\n",
    "    _, _, _, hyp_code_1, _ = model.get_learned_conditioning(prompt_1)\n",
    "    _, _, _, hyp_code_2, _ = model.get_learned_conditioning(prompt_2)\n",
    "    rescaled_hyp_code_1 = rescale(alpha, hyp_code_1)\n",
    "    rescaled_hyp_code_2 = rescale(alpha, hyp_code_2)\n",
    "    delta_hyp_code_1 = mobius_add(hyp_code_1, -rescaled_hyp_code_1)\n",
    "    delta_hyp_code_2 = mobius_add(hyp_code_2, -rescaled_hyp_code_2)\n",
    "    fused_hyp_code = mobius_add(rescaled_hyp_code_1, delta_hyp_code_2)\n",
    "    return fused_hyp_code\n",
    "\n",
    "def get_hyp_codes(model, prompts):\n",
    "    # return latent codes in the hyperbolic space for the given prompts\n",
    "    _, _, feature, feature_dist, _ = model.get_learned_conditioning(prompts)\n",
    "    return feature, feature_dist\n",
    "\n",
    "def get_hyp_codes_given_feature(model, feature):\n",
    "    # return latent codes in the hyperbolic space for the given latent codes in CLIP space\n",
    "    _, _, feature, feature_dist, _ = model.get_learned_conditioning(feature, input_feature=False, input_code=True)\n",
    "    return feature, feature_dist\n",
    "\n",
    "def get_condition_given_feature(model, feature):\n",
    "    # return latent codes in the CLIP space for the given latent codes in hyperbolic space\n",
    "    _, _, _, _, feature_euc = model.get_learned_conditioning(feature, input_feature=False, input_code=True)\n",
    "    return feature_euc\n",
    "\n",
    "def get_condition_given_hyp_codes(model, hyp_codes):\n",
    "    # return latent codes in the CLIP space for the given latent codes in hyperbolic space\n",
    "    _, _, _, _, feature_euc = model.get_learned_conditioning(hyp_codes, input_feature=True)\n",
    "    return feature_euc\n",
    "\n",
    "\n",
    "# rescale function\n",
    "def rescale(target_radius, x):\n",
    "    r_change = target_radius / \\\n",
    "        dist0(gmath.mobius_scalar_mul(\n",
    "            r=torch.tensor(1), x=x, k=torch.tensor(-1.0)))\n",
    "    return gmath.mobius_scalar_mul(r=r_change, x=x, k=torch.tensor(-1.0))\n",
    "\n",
    "\n",
    "# function for generating images with fixed radius (also contains raw geodesic images of 'shorten' images, and stretched images to boundary)\n",
    "def geo_interpolate_fix_r(x, y, interval, target_radius, save_codes=False):\n",
    "    feature_geo = []\n",
    "    feature_geo_normalized = []\n",
    "    dist_to_start = []\n",
    "    feature_geo_current_target_boundaries = []\n",
    "    target_radius_ratio = torch.tensor(target_radius/6.2126)\n",
    "    geodesic_start_short = gmath.mobius_scalar_mul(\n",
    "        r=target_radius_ratio, x=x, k=torch.tensor(-1.0))\n",
    "    geodesic_end_short = gmath.mobius_scalar_mul(\n",
    "        r=target_radius_ratio, x=y, k=torch.tensor(-1.0))\n",
    "    index = 0\n",
    "    for i in interval:\n",
    "        # this is raw image on geodesic, instead of fixed radius\n",
    "        feature_geo_current = gmath.geodesic(t=torch.tensor(\n",
    "            i), x=geodesic_start_short, y=geodesic_end_short, k=torch.tensor(-1.0))\n",
    "\n",
    "        # here we fix the radius and don't revert them now\n",
    "        r_change = target_radius / \\\n",
    "            dist0(gmath.mobius_scalar_mul(r=torch.tensor(1),\n",
    "                  x=feature_geo_current, k=torch.tensor(-1.0)))\n",
    "        feature_geo.append(feature_geo_current)\n",
    "        feature_geo_current_target_radius = gmath.mobius_scalar_mul(\n",
    "            r=r_change, x=feature_geo_current, k=torch.tensor(-1.0))\n",
    "        feature_geo_normalized.append(feature_geo_current_target_radius)\n",
    "        dist = gmath.dist(\n",
    "            geodesic_start_short, feature_geo_current_target_radius, k=torch.tensor(-1.0))\n",
    "        dist_to_start.append(dist)\n",
    "\n",
    "        # here is to revert the feature to boundary\n",
    "        r_change_to_boundary = 6.2126 / \\\n",
    "            dist0(gmath.mobius_scalar_mul(r=torch.tensor(1),\n",
    "                  x=feature_geo_current, k=torch.tensor(-1.0)))\n",
    "        feature_geo_current_target_boundary = gmath.mobius_scalar_mul(\n",
    "            r=r_change_to_boundary, x=feature_geo_current, k=torch.tensor(-1.0))\n",
    "        feature_geo_current_target_boundaries.append(feature_geo_current_target_boundary)\n",
    "\n",
    "    return feature_geo, feature_geo_normalized, feature_geo_current_target_boundaries, dist_to_start\n",
    "\n",
    "# function for generating images with fixed radius with optional latent codes list output\n",
    "\n",
    "\n",
    "def geo_interpolate_fix_r_with_codes(x, y, interval, target_radius):\n",
    "    # please use this with batch_size = 1\n",
    "    feature_geo = []\n",
    "    feature_geo_normalized = []\n",
    "    dist_to_start = []\n",
    "    target_radius_ratio = torch.tensor(target_radius/6.2126)\n",
    "    geodesic_start_short = gmath.mobius_scalar_mul(\n",
    "        r=target_radius_ratio, x=x, k=torch.tensor(-1.0))\n",
    "    geodesic_end_short = gmath.mobius_scalar_mul(\n",
    "        r=target_radius_ratio, x=y, k=torch.tensor(-1.0))\n",
    "    for i in interval:\n",
    "        # this is raw image on geodesic, instead of fixed radius\n",
    "        feature_geo_current = gmath.geodesic(t=torch.tensor(\n",
    "            i), x=geodesic_start_short, y=geodesic_end_short, k=torch.tensor(-1.0))\n",
    "\n",
    "        # here we fix the radius and don't revert them now\n",
    "        r_change = target_radius / \\\n",
    "            dist0(gmath.mobius_scalar_mul(r=torch.tensor(1),\n",
    "                  x=feature_geo_current, k=torch.tensor(-1.0)))\n",
    "        feature_geo.append(feature_geo_current)\n",
    "        feature_geo_current_target_radius = gmath.mobius_scalar_mul(\n",
    "            r=r_change, x=feature_geo_current, k=torch.tensor(-1.0))\n",
    "        feature_geo_normalized.append(feature_geo_current_target_radius)\n",
    "        dist = gmath.dist(\n",
    "            geodesic_start_short, feature_geo_current_target_radius, k=torch.tensor(-1.0))\n",
    "        dist_to_start.append(dist)\n",
    "        # print(feature_geo_current_target_radius.norm())\n",
    "\n",
    "        # here is to revert the feature to boundary\n",
    "        r_change_to_boundary = 6.2126 / \\\n",
    "            dist0(gmath.mobius_scalar_mul(r=torch.tensor(1),\n",
    "                  x=feature_geo_current, k=torch.tensor(-1.0)))\n",
    "        feature_geo_current_target_boundary = gmath.mobius_scalar_mul(\n",
    "            r=r_change_to_boundary, x=feature_geo_current, k=torch.tensor(-1.0))\n",
    "        # print(feature_geo_current_target_boundary.norm())\n",
    "\n",
    "    return dist_to_start, feature_geo_current, feature_geo_current_target_radius, feature_geo_current_target_boundary\n",
    "\n",
    "\n",
    "def geo_perturbation(x, distances, perturb_codes, target_radius=6.2126, num_samples=10, save_codes=False):\n",
    "    \"\"\"\n",
    "    该函数在双曲空间围绕给定的点 x 生成随机扰动，并确保扰动点的半径与 x 相同。\n",
    "    函数会为每个给定的双曲距离生成多个样本。\n",
    "\n",
    "    参数:\n",
    "        x (tensor): Poincaré圆盘中的起始点。\n",
    "        distances (list): 目标的双曲距离列表，例如 [0.01, 0.1, 0.2]。\n",
    "        target_radius (float): 要将扰动点缩放到的目标半径。\n",
    "        num_samples (int): 每个距离生成的样本数量。\n",
    "        save_codes (bool, 可选): 是否保存其他数据，默认为 False。\n",
    "    \n",
    "    返回:\n",
    "        tuple: 包含以下列表的元组：\n",
    "            - feature_geo: 未归一化的扰动点。\n",
    "            - feature_geo_normalized: 归一化到目标半径的扰动点。\n",
    "            - feature_geo_current_target_boundaries: 扰动点缩放到边界。\n",
    "            - dist_to_start: 扰动点与起始点 x 的双曲距离。\n",
    "            - perturbation_distances: 每个扰动点与起始点的双曲距离。\n",
    "    \"\"\"\n",
    "    \n",
    "    feature_geo = []\n",
    "    feature_geo_normalized = []\n",
    "    dist_to_start = []\n",
    "    feature_geo_current_target_boundaries = []\n",
    "    perturbation_distances = []  # 保存每个 feature_geo_current 和 geodesic_start_short 之间的距离\n",
    "    \n",
    "    # 1. 计算目标半径的比例，并缩放输入向量 x\n",
    "    target_radius_ratio = torch.tensor(target_radius / 6.2126)\n",
    "    \n",
    "    # 缩放 x 到目标半径，得到 geodesic_start_short\n",
    "    geodesic_start_short = gmath.mobius_scalar_mul(r=target_radius_ratio, x=x, k=torch.tensor(-1.0))\n",
    "    print('start_radius', dist0(gmath.mobius_scalar_mul(r=torch.tensor(1), x=geodesic_start_short, k=torch.tensor(-1.0))))\n",
    "    # 外层循环遍历每个距离\n",
    "    for distance in distances:\n",
    "        # 内层循环生成每个距离下的多个样本\n",
    "        for perturb_code in perturb_codes:\n",
    "            # 2. 生成随机方向\n",
    "            # random_direction = torch.randn_like(geodesic_start_short)/100\n",
    "            random_direction = perturb_code\n",
    "            \n",
    "            #print(dist0(gmath.mobius_scalar_mul(r=torch.tensor(1), x=random_direction, k=torch.tensor(-1.0))))\n",
    "            # 3. 使用 dist0 计算随机方向的双曲距离，并调整使其与 geodesic_start_short 一样长\n",
    "            r_change_direction_to = target_radius / dist0(gmath.mobius_scalar_mul(r=torch.tensor(1), x=random_direction, k=torch.tensor(-1.0)))\n",
    "            geodesic_end_short = gmath.mobius_scalar_mul(r=r_change_direction_to, x=random_direction, k=torch.tensor(-1.0))\n",
    "            # print(dist0(gmath.mobius_scalar_mul(r=torch.tensor(1), x=geodesic_end_short, k=torch.tensor(-1.0))))\n",
    "            # 4. 计算 geodesic_start_short 和 geodesic_end_short 之间的双曲距离\n",
    "            distance_x_y = gmath.dist(geodesic_start_short, geodesic_end_short, k=torch.tensor(-1.0))\n",
    "            # print('current_distance',distance_x_y)\n",
    "            # 5. 计算插值比例 t，确保 geodesic_start_short 和 feature_geo_current 之间的双曲距离等于指定的 distance\n",
    "            if distance_x_y > 0:\n",
    "                t = distance / distance_x_y  # 插值比例，确保按照双曲距离采样\n",
    "            else:\n",
    "                t = torch.tensor(1.0)  # 当距离极小时，设为1\n",
    "            \n",
    "            # 6. 沿着 geodesic_start_short 和 geodesic_end_short 插值生成 feature_geo_current\n",
    "            feature_geo_current = gmath.geodesic(t=t, x=geodesic_start_short, y=geodesic_end_short, k=torch.tensor(-1.0))\n",
    "            \n",
    "            # 7. 保存未归一化的 feature_geo_current\n",
    "            feature_geo.append(feature_geo_current)\n",
    "\n",
    "            # print('current_radius', dist0(gmath.mobius_scalar_mul(r=torch.tensor(1), x=feature_geo_current, k=torch.tensor(-1.0))))\n",
    "            # 8. 修正到目标半径，确保 feature_geo_current 的半径与 target_radius 一致\n",
    "            r_change = target_radius / dist0(gmath.mobius_scalar_mul(r=torch.tensor(1), x=feature_geo_current, k=torch.tensor(-1.0)))\n",
    "            # print('change ratio for interpolation sample',r_change)\n",
    "            feature_geo_current_target_radius = gmath.mobius_scalar_mul(r=r_change, x=feature_geo_current, k=torch.tensor(-1.0))\n",
    "            feature_geo_normalized.append(feature_geo_current_target_radius)\n",
    "            \n",
    "            # 9. 计算扰动点到起始点的双曲距离，并保存\n",
    "            dist = gmath.dist(geodesic_start_short, feature_geo_current_target_radius, k=torch.tensor(-1.0))\n",
    "            dist_to_start.append(dist)\n",
    "            \n",
    "            # 10. 保存每个 feature_geo_current 和 geodesic_start_short 的双曲距离\n",
    "            perturbation_distances.append(gmath.dist(geodesic_start_short, feature_geo_current, k=torch.tensor(-1.0)))\n",
    "            \n",
    "            # 11. 将扰动点调整到边界\n",
    "            r_change_to_boundary = 6.2126 / dist0(gmath.mobius_scalar_mul(r=torch.tensor(1), x=feature_geo_current, k=torch.tensor(-1.0)))\n",
    "            feature_geo_current_target_boundary = gmath.mobius_scalar_mul(r=r_change_to_boundary, x=feature_geo_current, k=torch.tensor(-1.0))\n",
    "            feature_geo_current_target_boundaries.append(feature_geo_current_target_boundary)\n",
    "    \n",
    "    return feature_geo, feature_geo_normalized, feature_geo_current_target_boundaries, dist_to_start, perturbation_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define animalfaces variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables\n",
    "init_image_path = './inputs/same_domain_test/animalfaces/cls_4_2.jpg'\n",
    "ref_image_path = './inputs/same_domain_test/animalfaces/cls_4_2.jpg'\n",
    "ref_image_2_path = './inputs/same_domain_test/animalfaces/cls_4_2.jpg'\n",
    "# sampling image\n",
    "files = glob.glob(\"/data2/mhf/DXL/Lingxiao/datasets/animals/*/*.jpg\")\n",
    "sampled_imgs = random.sample(files, 0)\n",
    "\n",
    "outdir = './outputs/animalfaces'\n",
    "skip_grid = False\n",
    "skip_save = True\n",
    "ddim_steps = 50\n",
    "ddim_eta = 0.0\n",
    "n_iter = 1\n",
    "C = 4\n",
    "f = 8\n",
    "n_samples = 5\n",
    "n_rows = 0\n",
    "scale = 1.3\n",
    "strength = 0.95\n",
    "config_path = './configs/stable-diffusion/v2_inference_animalfaces.yaml'\n",
    "ckpt = '/data2/mhf/DXL/Lingxiao/Codes/Paint-by-Example-test/models/Paint-by-Example/animal_faces/2024-09-11T11-30-12_v1/checkpoints/epoch=000037.ckpt'\n",
    "seed = 3408\n",
    "precision = 'autocast'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define ffhq variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables\n",
    "init_image_path = './inputs/same_domain_test/ffhq/2.png'\n",
    "ref_image_path = './inputs/same_domain_test/ffhq/2.png'\n",
    "# init_image_path = '/data2/mhf/DXL/Lingxiao/datasets/FFHQ/ffhq512/00009.png'\n",
    "# ref_image_path = '/data2/mhf/DXL/Lingxiao/datasets/FFHQ/ffhq512/00009.png'\n",
    "ref_image_2_path = './inputs/same_domain_test/ffhq/40.jpg'\n",
    "# sampling image\n",
    "files = glob.glob(\"/data2/mhf/DXL/Lingxiao/datasets/FFHQ/*/*.png\")\n",
    "sampled_imgs = random.sample(files, 2)\n",
    "\n",
    "outdir = './outputs/ffhq'\n",
    "skip_grid = False\n",
    "skip_save = False\n",
    "ddim_steps = 50\n",
    "ddim_eta = 0.0\n",
    "n_iter = 1\n",
    "C = 4\n",
    "f = 8\n",
    "n_samples = 5\n",
    "n_rows = 0\n",
    "scale = 1.3\n",
    "strength = 1.0\n",
    "config_path = './configs/stable-diffusion/v2_inference_ffhq.yaml'\n",
    "ckpt = '/data2/mhf/DXL/Lingxiao/Codes/Paint-by-Example-test/models/Paint-by-Example/vgg_faces/2024-10-02T02-12-51_v1/checkpoints/epoch=000037-ffhq.ckpt'\n",
    "seed = 3408\n",
    "precision = 'autocast'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define vggfaces variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables\n",
    "init_image_path = './inputs/same_domain_test/vggfaces/cls_4_2.jpg'\n",
    "ref_image_path = './inputs/same_domain_test/vggfaces/cls_4_2.jpg'\n",
    "ref_image_2_path = './inputs/same_domain_test/vggfaces/3.jpg'\n",
    "# sampling image\n",
    "files = glob.glob(\"/data2/mhf/DXL/Lingxiao/datasets/vggfaces/*/*.jpg\")\n",
    "sampled_imgs = random.sample(files, 2)\n",
    "\n",
    "outdir = './outputs/vggfaces'\n",
    "skip_grid = False\n",
    "skip_save = True\n",
    "ddim_steps = 50\n",
    "ddim_eta = 0.0\n",
    "n_iter = 1\n",
    "C = 4\n",
    "f = 8\n",
    "n_samples = 10\n",
    "n_rows = 0\n",
    "scale = 1.3\n",
    "strength = 0.95\n",
    "config_path = './configs/stable-diffusion/v2_inference_vggfaces.yaml'\n",
    "ckpt = '/data2/mhf/DXL/Lingxiao/Codes/Paint-by-Example-test/models/Paint-by-Example/vgg_faces/2024-10-02T02-12-51_v1/checkpoints/epoch=000020-vgg.ckpt'\n",
    "seed = 3408\n",
    "precision = 'autocast'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define flowers variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables\n",
    "init_image_path = './inputs/same_domain_test/flowers/cls_3_2.jpg'\n",
    "ref_image_path = './inputs/same_domain_test/flowers/cls_3_2.jpg'\n",
    "ref_image_2_path = './inputs/same_domain_test/flowers/cls_4_1.jpg'\n",
    "# sampling image\n",
    "files = glob.glob(\"/data2/mhf/DXL/Lingxiao/datasets/flowers/dataset/train/*/*.jpg\")\n",
    "sampled_imgs = random.sample(files, 10)\n",
    "\n",
    "outdir = './outputs/flowers'\n",
    "skip_grid = False\n",
    "skip_save = True\n",
    "ddim_steps = 50\n",
    "ddim_eta = 0.0\n",
    "n_iter = 1\n",
    "C = 4\n",
    "f = 8\n",
    "n_samples = 8\n",
    "n_rows = 0\n",
    "scale = 1.3\n",
    "strength = 1.0\n",
    "config_path = './configs/stable-diffusion/v2_inference_flowers.yaml'\n",
    "ckpt = '/data2/mhf/DXL/Lingxiao/Codes/Paint-by-Example-test/models/Paint-by-Example/flowers/2024-10-12T05-45-07_v1/checkpoints/epoch=000298.ckpt'\n",
    "seed = 3408\n",
    "precision = 'autocast'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define nabirds variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables\n",
    "init_image_path = './inputs/same_domain_test/nabirds/cls_1_1.jpg'\n",
    "ref_image_path = './inputs/same_domain_test/nabirds/cls_1_1.jpg'\n",
    "ref_image_2_path = './inputs/same_domain_test/nabirds/cls_2_1.jpg'\n",
    "# sampling image\n",
    "files = glob.glob(\"/data2/mhf/DXL/Lingxiao/datasets/nabirds/images/*/*.jpg\")\n",
    "sampled_imgs = random.sample(files, 0)\n",
    "\n",
    "outdir = './outputs/nabirds'\n",
    "skip_grid = False\n",
    "skip_save = True\n",
    "ddim_steps = 50\n",
    "ddim_eta = 0.0\n",
    "n_iter = 1\n",
    "C = 4\n",
    "f = 8\n",
    "n_samples = 5\n",
    "n_rows = 0\n",
    "scale = 1.3\n",
    "strength = 0.95\n",
    "config_path = './configs/stable-diffusion/v2_inference_nabirds.yaml'\n",
    "ckpt = '/data2/mhf/DXL/Lingxiao/Codes/Paint-by-Example-test/models/Paint-by-Example/nabirds/2024-10-17T10-15-57_v1/checkpoints/epoch=000083.ckpt'\n",
    "seed = 3408\n",
    "precision = 'autocast'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /data2/mhf/DXL/Lingxiao/Codes/Paint-by-Example-test/models/Paint-by-Example/animal_faces/2024-09-11T11-30-12_v1/checkpoints/epoch=000037.ckpt\n",
      "Global Step: 82620\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 865.91 M params.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data2/mhf/DXL/Lingxiao/Cache/huggingface/hub/models--openai--clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'visual_projection.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'logit_scale', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_projection.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use hyperbolic: True\n",
      "Loading HAE from checkpoint: /data2/mhf/DXL/Lingxiao/Codes/hyperediting/exp_out/hyper_animalfaces_512_5_30_init_v1/checkpoints/iteration_29000.pt\n",
      "Successfully loaded model!\n"
     ]
    }
   ],
   "source": [
    "# seed_everything(seed)\n",
    "\n",
    "config = OmegaConf.load(f\"{config_path}\")\n",
    "model = load_model_from_config(config, f\"{ckpt}\")\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") \n",
    "model = model.to(device)\n",
    "print(\"Successfully loaded model!\")\n",
    "\n",
    "sampler = DDIMSampler(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Delta Mapper for text-guided editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data2/mhf/DXL/Lingxiao/Cache/huggingface/hub/models--openai--clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'logit_scale', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.final_layer_norm.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_projection.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'visual_projection.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.bias']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use hyperbolic: True\n",
      "Loading HAE from checkpoint: /data2/mhf/DXL/Lingxiao/Codes/DeltaHyperEditing/exp_out/hyper_ffhq_512_5_30_v2/checkpoints/iteration_62500.pt\n",
      "Model successfully loaded!\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "sys.path.insert(0, '/data2/mhf/DXL/Lingxiao/Codes')\n",
    "from DeltaHyperEditing.models.delta_hyp_clip import hae_clip\n",
    "\n",
    "# model_path = '/data2/mhf/DXL/Lingxiao/Codes/DeltaHyperEditing/exp_out/hyper_animalfaces_512_5_30_v2/checkpoints/iteration_52500.pt'\n",
    "model_path = '/data2/mhf/DXL/Lingxiao/Codes/DeltaHyperEditing/exp_out/hyper_ffhq_512_5_30_v2/checkpoints/iteration_62500.pt'\n",
    "# model_path = '/data2/mhf/DXL/Lingxiao/Codes/DeltaHyperEditing/exp_out/hyper_flowers_512_5_5_v1/checkpoints/iteration_102500.pt'\n",
    "# model_path = '/data2/mhf/DXL/Lingxiao/Codes/DeltaHyperEditing/exp_out/hyper_nabirds_512_5_30_v1/checkpoints/iteration_162500.pt'\n",
    "# model_path = '/data2/mhf/DXL/Lingxiao/Codes/DeltaHyperEditing/exp_out/hyper_vggfaces_512_5_30_v2/checkpoints/iteration_62500.pt'\n",
    "\n",
    "ckpt = torch.load(model_path, map_location='cpu')\n",
    "opts = ckpt['opts']\n",
    "del ckpt\n",
    "opts['checkpoint_path'] = model_path\n",
    "opts['load_mapper'] = True\n",
    "# instantialize model with checkpoints and args\n",
    "opts = Namespace(**opts)\n",
    "net = hae_clip(opts)\n",
    "net.eval()\n",
    "net.to(device)\n",
    "print('Model successfully loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(outdir, exist_ok=True)\n",
    "outpath = outdir\n",
    "\n",
    "batch_size = n_samples\n",
    "n_rows = n_rows if n_rows > 0 else batch_size\n",
    "\n",
    "sample_path = os.path.join(outpath, \"samples\")\n",
    "os.makedirs(sample_path, exist_ok=True)\n",
    "base_count = len(os.listdir(sample_path))\n",
    "grid_count = len(os.listdir(outpath)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images\n",
    "# load init image\n",
    "assert os.path.isfile(init_image_path)\n",
    "init_image = load_img(init_image_path, [512, 512]).to(device)\n",
    "init_image_resized = load_img(init_image_path, [224, 224]).to(device)\n",
    "init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
    "init_image_resized = repeat(init_image_resized, '1 ... -> b ...', b=batch_size)\n",
    "init_latent = model.get_first_stage_encoding(\n",
    "    model.encode_first_stage(init_image))  # move to latent space\n",
    "\n",
    "# load ref image\n",
    "assert os.path.isfile(ref_image_path)\n",
    "ref_image = load_img(ref_image_path, [224, 224]).to(device)\n",
    "ref_image_resized = load_img(ref_image_path, [512, 512]).to(device)\n",
    "ref_image = repeat(ref_image, '1 ... -> b ...', b=batch_size)\n",
    "ref_image_resized = repeat(ref_image_resized, '1 ... -> b ...', b=batch_size)\n",
    "init_ref_latent = model.get_first_stage_encoding(\n",
    "    model.encode_first_stage(ref_image_resized))  # move to latent space\n",
    "\n",
    "assert os.path.isfile(ref_image_2_path)\n",
    "ref_image_2 = load_img(ref_image_2_path, [224, 224]).to(device)\n",
    "ref_image_2 = repeat(ref_image_2, '1 ... -> b ...', b=batch_size)\n",
    "\n",
    "# load sampled images\n",
    "sampled_images = []\n",
    "for sampled_img in sampled_imgs:\n",
    "    assert os.path.isfile(sampled_img)\n",
    "    sampled_image = load_img(sampled_img, [224, 224]).to(device)\n",
    "    sampled_images.append(sampled_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit latent codes using text instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'a face of a baby'\n",
    "prompt_delta = 'a crying face of a baby'\n",
    "# prompt_delta = 'a smiling face of a baby'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake_delta_s shape: torch.Size([10, 1, 512])\n",
      "feature shape: torch.Size([10, 1, 512])\n",
      "reconstruct_code shape: torch.Size([10, 1, 512])\n"
     ]
    }
   ],
   "source": [
    "fake_delta_s = net.get_fake_delta_s_given_data(ref_image, prompt, prompt_delta, feature_type='hyperbolic', device=device)\n",
    "print(f\"fake_delta_s shape: {fake_delta_s.shape}\")\n",
    "logits, ocodes, feature, feature_dist, feature_euc = model.get_learned_conditioning(ref_image)\n",
    "_, ocodes_2, feature_2, feature_dist_2, feature_euc_2 = model.get_learned_conditioning(ref_image_2)\n",
    "print(f\"feature shape: {feature.shape}\")\n",
    "reconstruct_code = feature + fake_delta_s\n",
    "# reconstruct_code = feature_dist + fake_delta_s\n",
    "print(f\"reconstruct_code shape: {reconstruct_code.shape}\")\n",
    "reconstruct_feature = get_condition_given_feature(model, reconstruct_code)\n",
    "reconstruct_codes = [feature_euc, reconstruct_feature]\n",
    "# reconstruct_codes = [feature_euc, reconstruct_code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512])\n",
      "tensor([6.2126], device='cuda:0', grad_fn=<MulBackward1>)\n"
     ]
    }
   ],
   "source": [
    "_, hyp_codes = get_hyp_codes_given_feature(model, reconstruct_code)\n",
    "print(hyp_codes.shape)\n",
    "hyp_code = hyp_codes[0].unsqueeze(0)\n",
    "print(gmath.dist0(hyp_code, k=torch.tensor(-1.0)))\n",
    "rescaled_codes = []\n",
    "target_radii = [6.2126, 4, 2.5, 1, 0.5, 0]\n",
    "for i in target_radii:\n",
    "    hyp_code_rescaled = rescale(i, hyp_code)\n",
    "    hyp_code_rescaled = repeat(hyp_code_rescaled, '1 ... -> b ...', b=batch_size)\n",
    "    feature_euc = get_condition_given_hyp_codes(model, hyp_code_rescaled)\n",
    "    rescaled_codes.append(feature_euc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_codes = []\n",
    "interval = [0, 0.3, 0.4, 0.5, 0.6, 0.7, 1]\n",
    "for i in interval:\n",
    "    feature = (1-i) * feature_euc + i * reconstruct_feature\n",
    "    interpolated_codes.append(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulate latent codes in Hyperbolic space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving latent codes from edge to center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n",
      "tensor([6.2126], device='cuda:0', grad_fn=<MulBackward1>)\n"
     ]
    }
   ],
   "source": [
    "_, hyp_code = get_hyp_codes(model, ref_image[0].unsqueeze(0))\n",
    "print(hyp_code.shape)\n",
    "print(gmath.dist0(hyp_code, k=torch.tensor(-1.0)))\n",
    "# this is used for generating figure of varying radius in our paper\n",
    "rescaled_codes = []\n",
    "target_radii = [6.2126, 5, 4, 3, 2.5, 1, 0.5, 0]\n",
    "for i in target_radii:\n",
    "    hyp_code_rescaled = rescale(i, hyp_code)\n",
    "    hyp_code_rescaled = repeat(hyp_code_rescaled, '1 ... -> b ...', b=batch_size)\n",
    "    feature_euc = get_condition_given_hyp_codes(model, hyp_code_rescaled)\n",
    "    rescaled_codes.append(feature_euc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate latent codes in Hyperbolic space along the geodesic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n",
      "tensor([6.2126], device='cuda:0', grad_fn=<MulBackward1>)\n"
     ]
    }
   ],
   "source": [
    "_, hyp_code = get_hyp_codes(model, ref_image[0].unsqueeze(0))\n",
    "_, hyp_code_2 = get_hyp_codes(model, ref_image_2[0].unsqueeze(0))\n",
    "print(hyp_code.shape)\n",
    "print(gmath.dist0(hyp_code, k=torch.tensor(-1.0)))\n",
    "# this is used for generating figure of varying radius in our paper\n",
    "interpolated_codes = []\n",
    "interval = [0, 0.3, 0.4, 0.45, 0.48, 0.49, 0.495, 0.50, 0.505, 0.51, 0.52, 0.55, 0.6, 0.7, 0.8, 1]\n",
    "feature_geo, feature_geo_current_target_radius, feature_geo_current_target_boundary, dist_to_start = geo_interpolate_fix_r(\n",
    "    hyp_code, hyp_code_2, interval, target_radius=6.2126)\n",
    "for i in feature_geo_current_target_boundary:\n",
    "    feature = repeat(i, '1 ... -> b ...', b=batch_size)\n",
    "    feature_euc = get_condition_given_hyp_codes(model, feature)\n",
    "    interpolated_codes.append(feature_euc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature fusion in hyperbolic space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_codes = []\n",
    "fused_scale = [0, 0.45, 0.8, 1.0, 1.5, 2.0, 3, 5, 6.2126]\n",
    "for scale in fused_scale:\n",
    "    fused_hyp_code = feature_fusion(model, ref_image[0].unsqueeze(0), ref_image_2[0].unsqueeze(0), scale)\n",
    "    fused_hyp_code = repeat(fused_hyp_code, '1 ... -> b ...', b=batch_size)\n",
    "    fused_feature_euc = get_condition_given_hyp_codes(model, fused_hyp_code)\n",
    "    fused_codes.append(fused_feature_euc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perturbation given an image embedding with certain radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n",
      "tensor([6.2126], device='cuda:0', grad_fn=<MulBackward1>)\n",
      "start_radius tensor([6.2126], device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "perturbed_codes = []\n",
    "logits, ocodes, feature, feature_dist, feature_euc = model.get_learned_conditioning(ref_image)\n",
    "perturbed_codes.append(feature_euc)\n",
    "_, hyp_code = get_hyp_codes(model, ref_image[0].unsqueeze(0))\n",
    "_, perturb_codes = get_hyp_codes(model, sampled_images)\n",
    "print(hyp_code.shape)\n",
    "print(gmath.dist0(hyp_code, k=torch.tensor(-1.0)))\n",
    "distances = [4.0]\n",
    "num_samples = len(sampled_images)\n",
    "feature_geo, feature_geo_normalized, feature_geo_current_target_boundaries, dist_to_start, perturbation_distances = geo_perturbation(\n",
    "    hyp_code, distances, target_radius=6.2126, perturb_codes=perturb_codes, num_samples=num_samples)\n",
    "for i in feature_geo_current_target_boundaries:\n",
    "    feature = repeat(i, '1 ... -> b ...', b=batch_size)\n",
    "    feature_euc = get_condition_given_hyp_codes(model, feature)\n",
    "    perturbed_codes.append(feature_euc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulate latent codes in Euclidean space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate latent codes in Euclidean space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_euc shape: torch.Size([5, 1, 1024])\n"
     ]
    }
   ],
   "source": [
    "logits, ocodes, feature, feature_dist, feature_euc = model.get_learned_conditioning(ref_image)\n",
    "_, ocodes, feature_2, feature_dist_2, feature_euc_2 = model.get_learned_conditioning(ref_image_2)\n",
    "print('feature_euc shape:', feature_euc.shape)\n",
    "interpolated_codes = []\n",
    "interval = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "for i in interval:\n",
    "    feature = (1-i) * feature_euc + i * feature_euc_2\n",
    "    interpolated_codes.append(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Images before and after hyperbolic space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_euc shape: torch.Size([4, 1, 1024])\n",
      "ocodes shape: torch.Size([4, 1, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/mhf/DXL/Lingxiao/Codes/HypDiffusion/ldm/modules/encoders/hyper_nets.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  k = torch.tensor(k)\n"
     ]
    }
   ],
   "source": [
    "logits, ocodes, feature, feature_dist, feature_euc = model.get_learned_conditioning(ref_image)\n",
    "print('feature_euc shape:', feature_euc.shape)\n",
    "print('ocodes shape:', ocodes.shape)\n",
    "compare_codes = []\n",
    "compare_codes.append(ocodes)\n",
    "compare_codes.append(feature_euc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the condition only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_euc shape: torch.Size([5, 1, 1024])\n",
      "ocodes shape: torch.Size([5, 1, 1024])\n"
     ]
    }
   ],
   "source": [
    "logits, ocodes, feature, feature_dist, feature_euc = model.get_learned_conditioning(ref_image)\n",
    "print('feature_euc shape:', feature_euc.shape)\n",
    "print('ocodes shape:', ocodes.shape)\n",
    "code = []\n",
    "code.append(feature_euc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1.3\n",
    "strength = 1.0\n",
    "skip_save = True\n",
    "change_noise = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target t_enc is 50 steps\n",
      "Data shape for DDIM sampling is (5, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:16<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid saved to ./outputs/animalfaces/grid-0133.png\n",
      "Your samples are ready and waiting for you here: \n",
      "./outputs/animalfaces \n",
      " \n",
      "Enjoy.\n"
     ]
    }
   ],
   "source": [
    "sampler.make_schedule(ddim_num_steps=ddim_steps,\n",
    "                      ddim_eta=ddim_eta, verbose=False)\n",
    "assert 0. <= strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
    "t_enc = int(strength * ddim_steps)\n",
    "print(f\"target t_enc is {t_enc} steps\")\n",
    "\n",
    "precision_scope = autocast if precision == \"autocast\" else nullcontext\n",
    "n_rows = n_rows if n_rows > 0 else batch_size\n",
    "uc = get_unconditional_embedding(model, scale, n_samples, device, ref_image)\n",
    "\n",
    "with torch.no_grad():\n",
    "    with precision_scope(\"cuda\"):\n",
    "        with model.ema_scope():\n",
    "            tic = time.time()\n",
    "            all_samples = list()\n",
    "            shape = [C, 64, 64]\n",
    "            if not change_noise:\n",
    "                if strength < 1.0:\n",
    "                    z_enc = sampler.stochastic_encode(\n",
    "                        init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
    "                # print(f\"z_enc shape: {z_enc.shape}\")\n",
    "                else:\n",
    "                    z_enc = torch.randn([n_samples, 4, 64, 64], device=device)\n",
    "            # decode it\n",
    "            # for c in rescaled_codes:\n",
    "            # for c in interpolated_codes:\n",
    "            # for c in reconstruct_codes:\n",
    "            # for c in perturbed_codes:\n",
    "            # for c in compare_codes:\n",
    "            # for c in fused_codes:\n",
    "            for c in code:\n",
    "            # encode (scaled latent)\n",
    "                if change_noise:\n",
    "                    if strength < 1.0:\n",
    "                        z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
    "                    # print(f\"z_enc shape: {z_enc.shape}\")\n",
    "                    else:\n",
    "                        z_enc = torch.randn([n_samples, 4, 64, 64], device=device)\n",
    "                    \n",
    "                if strength == 1.0:\n",
    "                    samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
    "                                                        conditioning=c,\n",
    "                                                        batch_size=n_samples,\n",
    "                                                        shape=shape,\n",
    "                                                        verbose=False,\n",
    "                                                        unconditional_guidance_scale=scale,\n",
    "                                                        unconditional_conditioning=uc,\n",
    "                                                        eta=ddim_eta,\n",
    "                                                        x_T=z_enc)\n",
    "                else:\n",
    "                    samples_ddim = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=scale,\n",
    "                                                unconditional_conditioning=uc)\n",
    "                                            \n",
    "\n",
    "                x_samples = model.decode_first_stage(samples_ddim)\n",
    "                x_samples = torch.clamp(\n",
    "                    (x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "                if not skip_save:\n",
    "                    for x_sample in x_samples:\n",
    "                        x_sample = 255. * \\\n",
    "                            rearrange(x_sample.cpu().numpy(),\n",
    "                                        'c h w -> h w c')\n",
    "                        Image.fromarray(x_sample.astype(np.uint8)).save(\n",
    "                            os.path.join(sample_path, f\"{base_count:05}.png\"))\n",
    "                        base_count += 1\n",
    "                all_samples.append(x_samples)\n",
    "\n",
    "                if not skip_grid:\n",
    "                    # additionally, save as grid\n",
    "                    grid = torch.stack(all_samples, 0)\n",
    "                    grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "                    grid = make_grid(grid, nrow=n_rows)\n",
    "\n",
    "            # to image\n",
    "            grid = 255. * \\\n",
    "                rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "            Image.fromarray(grid.astype(np.uint8)).save(\n",
    "                os.path.join(outpath, f'grid-{grid_count:04}.png'))\n",
    "            print(f\"grid saved to {outpath}/grid-{grid_count:04}.png\")\n",
    "            grid_count += 1\n",
    "            del grid\n",
    "\n",
    "        toc = time.time()\n",
    "\n",
    "print(f\"Your samples are ready and waiting for you here: \\n{outpath} \\n\"\n",
    "        f\" \\nEnjoy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean of the variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target t_enc is 30 steps\n",
      "Data shape for DDIM sampling is (20, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:33<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 512, 512])\n",
      "Data shape for DDIM sampling is (20, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:33<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 512, 512])\n",
      "Data shape for DDIM sampling is (20, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:33<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 512, 512])\n",
      "Data shape for DDIM sampling is (20, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:33<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 512, 512])\n",
      "Data shape for DDIM sampling is (20, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:33<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 512, 512])\n",
      "Data shape for DDIM sampling is (20, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:33<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 512, 512])\n",
      "Your samples are ready and waiting for you here: \n",
      "./outputs/ffhq \n",
      " \n",
      "Enjoy.\n"
     ]
    }
   ],
   "source": [
    "sampler.make_schedule(ddim_num_steps=ddim_steps,\n",
    "                      ddim_eta=ddim_eta, verbose=False)\n",
    "assert 0. <= strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
    "t_enc = int(strength * ddim_steps)\n",
    "print(f\"target t_enc is {t_enc} steps\")\n",
    "\n",
    "precision_scope = autocast if precision == \"autocast\" else nullcontext\n",
    "n_rows = n_rows if n_rows > 0 else batch_size\n",
    "uc = get_unconditional_embedding(model, scale, n_samples, device, ref_image)\n",
    "\n",
    "with torch.no_grad():\n",
    "    with precision_scope(\"cuda\"):\n",
    "        with model.ema_scope():\n",
    "            tic = time.time()\n",
    "            all_samples = list()\n",
    "            shape = [C, 64, 64]\n",
    "            # encode (scaled latent)\n",
    "            if strength < 1.0:\n",
    "                z_enc = sampler.stochastic_encode(\n",
    "                    init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
    "            # print(f\"z_enc shape: {z_enc.shape}\")\n",
    "            else:\n",
    "                z_enc = torch.randn([n_samples, 4, 64, 64], device=device)\n",
    "            # decode it\n",
    "            for c in rescaled_codes:\n",
    "                samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
    "                                                 conditioning=c,\n",
    "                                                 batch_size=n_samples,\n",
    "                                                 shape=shape,\n",
    "                                                 verbose=False,\n",
    "                                                 unconditional_guidance_scale=scale,\n",
    "                                                 unconditional_conditioning=uc,\n",
    "                                                 eta=ddim_eta,\n",
    "                                                 x_T=z_enc)\n",
    "                '''\n",
    "                samples_ddim = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=scale,\n",
    "                                            unconditional_conditioning=uc,)\n",
    "                                            '''\n",
    "\n",
    "                x_samples = model.decode_first_stage(samples_ddim)\n",
    "                x_samples = torch.clamp(\n",
    "                    (x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "                print(x_samples.shape)\n",
    "                x_samples_mean = x_samples.mean(0).unsqueeze(0)\n",
    "                if not skip_save:\n",
    "                    for x_sample in x_samples:\n",
    "                        x_sample = 255. * \\\n",
    "                            rearrange(x_sample.cpu().numpy(),\n",
    "                                      'c h w -> h w c')\n",
    "                        Image.fromarray(x_sample.astype(np.uint8)).save(\n",
    "                            os.path.join(sample_path, f\"{base_count:05}.png\"))\n",
    "                        base_count += 1\n",
    "                all_samples.append(x_samples_mean)\n",
    "\n",
    "                if not skip_grid:\n",
    "                    # additionally, save as grid\n",
    "                    grid = torch.stack(all_samples, 0)\n",
    "                    grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "                    grid = make_grid(grid, nrow=n_rows)\n",
    "\n",
    "            # to image\n",
    "            grid = 255. * \\\n",
    "                rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "            Image.fromarray(grid.astype(np.uint8)).save(\n",
    "                os.path.join(outpath, f'grid-{grid_count:04}.png'))\n",
    "            grid_count += 1\n",
    "            del grid\n",
    "\n",
    "        toc = time.time()\n",
    "\n",
    "print(f\"Your samples are ready and waiting for you here: \\n{outpath} \\n\"\n",
    "      f\" \\nEnjoy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define neccessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "IMG_EXTENSIONS = [\n",
    "    '.jpg', '.JPG', '.jpeg', '.JPEG',\n",
    "    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP', '.tiff'\n",
    "]\n",
    "\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
    "\n",
    "\n",
    "def make_dataset(dir):\n",
    "    images = []\n",
    "    assert os.path.isdir(dir), '%s is not a valid directory' % dir\n",
    "    for root, _, fnames in sorted(os.walk(dir)):\n",
    "        for fname in fnames:\n",
    "            if is_image_file(fname):\n",
    "                path = os.path.join(root, fname)\n",
    "                images.append(path)\n",
    "    return images\n",
    "\n",
    "\n",
    "def find_classes(directory):\n",
    "    \"\"\"Finds the class folders in a dataset.\n",
    "    See :class:`DatasetFolder` for details.\n",
    "    \"\"\"\n",
    "    classes = sorted(entry.name for entry in os.scandir(\n",
    "        directory) if entry.is_dir())\n",
    "    if not classes:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Couldn't find any class folder in {directory}.\")\n",
    "\n",
    "    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "    return classes, class_to_idx\n",
    "\n",
    "\n",
    "def copy_dataset(path, new_path):\n",
    "    # copy all images in this dictionary to new_path\n",
    "    images = make_dataset(path)\n",
    "    print(len(images))\n",
    "    for image in tqdm(images):\n",
    "        img = cv2.imread(image)\n",
    "        # print(image)\n",
    "        image_name = image.split('/')[-2] + '_' + image.split('/')[-1]\n",
    "        if not os.path.exists(new_path):\n",
    "            os.makedirs(new_path)\n",
    "        if img is not None:\n",
    "            cv2.imwrite(new_path + '/' + image_name, img)\n",
    "            \n",
    "\n",
    "def copy_dataset_new(path, new_path):\n",
    "    # copy all images in this dictionary to new_path\n",
    "    classes, _ = find_classes(path)\n",
    "    for class_name in tqdm(classes):\n",
    "        class_path = os.path.join(path, class_name)\n",
    "        new_class_path = os.path.join(new_path, class_name)\n",
    "        images = make_dataset(class_path)\n",
    "        for image in images:\n",
    "            img = cv2.imread(image)\n",
    "            if not os.path.exists(new_class_path):\n",
    "                os.makedirs(new_class_path)\n",
    "            if img is not None:\n",
    "                cv2.imwrite(new_class_path + '/' + image.split('/')[-1], img)\n",
    "            \n",
    "\n",
    "def sample_dataset(path, new_path, size):\n",
    "    images = make_dataset(path)\n",
    "    sample_images = random.sample(images, k=size)\n",
    "    # print(sample_images)\n",
    "\n",
    "    for image in tqdm(sample_images):\n",
    "        img = cv2.imread(image)\n",
    "        # print(img)\n",
    "        dirs = new_path\n",
    "        if not os.path.exists(dirs):\n",
    "            os.makedirs(dirs)\n",
    "        if img is not None:\n",
    "            cv2.imwrite(dirs + '/' + image.split('/')[-1], img)\n",
    "            \n",
    "\n",
    "def select_subset(path, new_path, num_classes, num_samples_per_class=None):\n",
    "    classes, class_to_idx = find_classes(path)\n",
    "    test_classes = random.sample(classes, k=num_classes)\n",
    "    train_classes = set(classes)-set(test_classes)\n",
    "    print(len(test_classes))\n",
    "    print(len(train_classes))\n",
    "    for i in range(num_classes):\n",
    "        test_class = classes[i]\n",
    "        source_dir = path + '/' + str(test_class)\n",
    "        destination_dir = new_path + '/' + str(test_class)\n",
    "        if num_samples_per_class is not None:\n",
    "            sample_dataset(source_dir, destination_dir, num_samples_per_class)\n",
    "        else:\n",
    "            shutil.copytree(source_dir, destination_dir)\n",
    "        print('finish {}/{}'.format(i, num_classes))\n",
    "    '''\n",
    "    for test_class in tqdm(test_classes):\n",
    "        source_dir = path + 'valid/' + str(test_class)\n",
    "        destination_dir = new_path + 'test/' + str(test_class)\n",
    "        shutil.copytree(source_dir, destination_dir)\n",
    "        '''\n",
    "    print(\"Finish creating new dataset!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "119\n",
      "finish 0/30\n",
      "finish 1/30\n",
      "finish 2/30\n",
      "finish 3/30\n",
      "finish 4/30\n",
      "finish 5/30\n",
      "finish 6/30\n",
      "finish 7/30\n",
      "finish 8/30\n",
      "finish 9/30\n",
      "finish 10/30\n",
      "finish 11/30\n",
      "finish 12/30\n",
      "finish 13/30\n",
      "finish 14/30\n",
      "finish 15/30\n",
      "finish 16/30\n",
      "finish 17/30\n",
      "finish 18/30\n",
      "finish 19/30\n",
      "finish 20/30\n",
      "finish 21/30\n",
      "finish 22/30\n",
      "finish 23/30\n",
      "finish 24/30\n",
      "finish 25/30\n",
      "finish 26/30\n",
      "finish 27/30\n",
      "finish 28/30\n",
      "finish 29/30\n",
      "Finish creating new dataset!\n"
     ]
    }
   ],
   "source": [
    "dataset_path = '/data2/mhf/DXL/Lingxiao/datasets/animals'\n",
    "select_subset(dataset_path, '/data2/mhf/DXL/Lingxiao/datasets/animals_eva_random', 30, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "913\n"
     ]
    }
   ],
   "source": [
    "test123 = '/data2/mhf/DXL/Lingxiao/datasets/animals/n02085620'\n",
    "images = make_dataset(test123)\n",
    "print(len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 33.54it/s]\n"
     ]
    }
   ],
   "source": [
    "sample_dataset(\n",
    "    '/data2/mhf/DXL/Lingxiao/datasets/nabirds_eva/test/0299', '/data2/mhf/DXL/Lingxiao/datasets/nabirds_eva_30/0299', 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = '/data2/mhf/DXL/Lingxiao/datasets/flowers/dataset/train_eva'\n",
    "outdir = './outputs/flowers_eva_genrated_test_cfg_1.7_strength_0.9'\n",
    "skip_save = False\n",
    "ddim_steps = 50\n",
    "ddim_eta = 0.0\n",
    "n_iter = 1\n",
    "C = 4\n",
    "f = 8\n",
    "n_samples = 16\n",
    "total_samples = 128\n",
    "n_rows = 0\n",
    "scale = 1.7\n",
    "strength = 0.9\n",
    "precision = 'autocast'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample images for calculating LPIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:44<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 1/19\n",
      "38\n",
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:44<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:45<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 2/19\n",
      "35\n",
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:44<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:44<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 3/19\n",
      "49\n",
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:44<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:45<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:44<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 4/19\n",
      "36\n",
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:45<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:44<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 5/19\n",
      "68\n",
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:44<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:44<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:44<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:44<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 6/19\n",
      "73\n",
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:44<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:45<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:42<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:41<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 7/19\n",
      "38\n",
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:41<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:45<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 8/19\n",
      "44\n",
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:44<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:45<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 9/19\n",
      "38\n",
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:44<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:44<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 10/19\n",
      "36\n",
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:44<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:44<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 11/19\n",
      "60\n",
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:44<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:44<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:44<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 12/19\n",
      "65\n",
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:45<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:44<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:44<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:45<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 13/19\n",
      "38\n",
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:44<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:44<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 14/19\n",
      "49\n",
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:45<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:45<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:44<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 15/19\n",
      "46\n",
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:45<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:44<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 16/19\n",
      "34\n",
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:45<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:44<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 17/19\n",
      "47\n",
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:44<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:44<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 18/19\n",
      "72\n",
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:45<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:44<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:44<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:44<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 19/19\n",
      "Your samples are ready and waiting for you \n",
      " \n",
      "Enjoy.\n"
     ]
    }
   ],
   "source": [
    "sampler.make_schedule(ddim_num_steps=ddim_steps,\n",
    "                      ddim_eta=ddim_eta, verbose=False)\n",
    "assert 0. <= strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
    "precision_scope = autocast if precision == \"autocast\" else nullcontext\n",
    "batch_size = n_samples\n",
    "n_rows = n_rows if n_rows > 0 else batch_size\n",
    "classes, class_to_idx = find_classes(image_folder)\n",
    "count = 0\n",
    "for class_name in classes:\n",
    "    class_path = os.path.join(image_folder, class_name)\n",
    "    images = make_dataset(class_path)\n",
    "    print(len(images))\n",
    "    for i in range(int(len(images)/batch_size)):\n",
    "        input_images = []\n",
    "        input_images_resized = []\n",
    "        for m in range(i*batch_size, (i+1)*batch_size):\n",
    "            image_path = images[m]\n",
    "            assert os.path.isfile(image_path)\n",
    "            init_image = load_img(image_path, [512, 512]).to(device)\n",
    "            init_image_resized = load_img(image_path, [224, 224]).to(device)\n",
    "            input_images.append(init_image)\n",
    "            input_images_resized.append(init_image_resized)\n",
    "            \n",
    "        init_image = torch.stack(input_images).squeeze(1)\n",
    "        ref_image = torch.stack(input_images_resized).squeeze(1)\n",
    "        init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
    "        t_enc = int(strength * ddim_steps)\n",
    "        uc = get_unconditional_embedding(model, scale, n_samples, device, ref_image)\n",
    "        \n",
    "        logits, ocodes, feature, feature_dist, feature_euc = model.get_learned_conditioning(ref_image)\n",
    "        code = [feature_euc]\n",
    "        with torch.no_grad():\n",
    "            with precision_scope(\"cuda\"):\n",
    "                with model.ema_scope():\n",
    "                    tic = time.time()\n",
    "                    all_samples = list()\n",
    "                    shape = [C, 64, 64]\n",
    "                    # encode (scaled latent)\n",
    "                    if strength < 1.0:\n",
    "                        z_enc = sampler.stochastic_encode(\n",
    "                            init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
    "                    # print(f\"z_enc shape: {z_enc.shape}\")\n",
    "                    else:\n",
    "                        z_enc = torch.randn([n_samples, 4, 64, 64], device=device)\n",
    "                    # decode it\n",
    "                    # for c in rescaled_codes:\n",
    "                    # for c in interpolated_codes:\n",
    "                    # for c in reconstruct_codes:\n",
    "                    # for c in perturbed_codes:\n",
    "                    # for c in compare_codes:\n",
    "                    # for c in fused_codes:\n",
    "                    for c in code:\n",
    "                        if strength == 1.0:\n",
    "                            samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
    "                                                            conditioning=c,\n",
    "                                                            batch_size=n_samples,\n",
    "                                                            shape=shape,\n",
    "                                                            verbose=False,\n",
    "                                                            unconditional_guidance_scale=scale,\n",
    "                                                            unconditional_conditioning=uc,\n",
    "                                                            eta=ddim_eta,\n",
    "                                                            x_T=z_enc)\n",
    "                        else:\n",
    "                            samples_ddim = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=scale,\n",
    "                                                        unconditional_conditioning=uc)\n",
    "\n",
    "                        x_samples = model.decode_first_stage(samples_ddim)\n",
    "                        x_samples = torch.clamp(\n",
    "                            (x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "                        count_img = 0\n",
    "                        if not skip_save:\n",
    "                            for x_sample in x_samples:\n",
    "                                x_sample = 255. * \\\n",
    "                                    rearrange(x_sample.cpu().numpy(),\n",
    "                                            'c h w -> h w c')\n",
    "                                output_dir = os.path.join(outdir, images[i*batch_size+count_img].split('/')[-2])\n",
    "                                if not os.path.exists(output_dir):\n",
    "                                    os.makedirs(output_dir)\n",
    "                                Image.fromarray(x_sample.astype(np.uint8)).save(\n",
    "                                    os.path.join(output_dir, images[i*batch_size+count_img].split('/')[-1]))\n",
    "                                count_img += 1\n",
    "\n",
    "    toc = time.time()\n",
    "    count += 1\n",
    "    print('finish {}/{}'.format(count, len(classes)))\n",
    "\n",
    "print(f\"Your samples are ready and waiting for you \\n\"\n",
    "        f\" \\nEnjoy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample images for calculating FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = '/data2/mhf/DXL/Lingxiao/datasets/animals_eva/train'\n",
    "outdir = './outputs/animals_eva_genrated_cfg_1.3_strength_0.95_r_5.6_30_samples_30_each'\n",
    "files = glob.glob(\"/data2/mhf/DXL/Lingxiao/datasets/animals/n02085620/*.jpg\")\n",
    "skip_save = False\n",
    "ddim_steps = 50\n",
    "ddim_eta = 0.0\n",
    "n_iter = 1\n",
    "C = 4\n",
    "f = 8\n",
    "n_samples = 5\n",
    "n_perturb_samples = 5\n",
    "n_selected = 30\n",
    "n_rows = 0\n",
    "scale = 1.3\n",
    "strength = 0.95\n",
    "precision = 'autocast'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_radius tensor([6.2126], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 1/30\n",
      "start_radius tensor([6.2126], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:13<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:14<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:14<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:13<00:00,  3.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 2/30\n",
      "start_radius tensor([6.2126], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:14<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:14<00:00,  3.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:14<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 3/30\n",
      "start_radius tensor([6.2126], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 4/30\n",
      "start_radius tensor([6.2126], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 5/30\n",
      "start_radius tensor([6.2126], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 6/30\n",
      "start_radius tensor([6.2126], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 7/30\n",
      "start_radius tensor([6.2126], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 8/30\n",
      "start_radius tensor([6.2126], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 9/30\n",
      "start_radius tensor([6.2126], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 10/30\n",
      "start_radius tensor([6.2126], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 11/30\n",
      "start_radius tensor([6.2126], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 12/30\n",
      "start_radius tensor([6.2126], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 13/30\n",
      "start_radius tensor([6.2126], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 14/30\n",
      "start_radius tensor([6.2126], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 15/30\n",
      "start_radius tensor([6.2126], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 16/30\n",
      "start_radius tensor([6.2126], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 17/30\n",
      "start_radius tensor([6.2126], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 18/30\n",
      "start_radius tensor([6.2126], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 19/30\n",
      "start_radius tensor([6.2126], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 20/30\n",
      "start_radius tensor([6.2126], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 21/30\n",
      "start_radius tensor([6.2126], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 22/30\n",
      "start_radius tensor([6.2126], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 23/30\n",
      "start_radius tensor([6.2126], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 24/30\n",
      "start_radius tensor([6.2126], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 25/30\n",
      "start_radius tensor([6.2126], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 26/30\n",
      "start_radius tensor([6.2126], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 27/30\n",
      "start_radius tensor([6.2126], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 28/30\n",
      "start_radius tensor([6.2126], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 29/30\n",
      "start_radius tensor([6.2126], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 47 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 47/47 [00:15<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 30/30\n",
      "Your samples are ready and waiting for you \n",
      " \n",
      "Enjoy.\n"
     ]
    }
   ],
   "source": [
    "sampler.make_schedule(ddim_num_steps=ddim_steps,\n",
    "                      ddim_eta=ddim_eta, verbose=False)\n",
    "assert 0. <= strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
    "precision_scope = autocast if precision == \"autocast\" else nullcontext\n",
    "batch_size = n_samples\n",
    "n_rows = n_rows if n_rows > 0 else batch_size\n",
    "classes, class_to_idx = find_classes(image_folder)\n",
    "count = 0\n",
    "for class_name in classes:\n",
    "    class_path = os.path.join(image_folder, class_name)\n",
    "    images = make_dataset(class_path)\n",
    "    if n_selected < len(images):\n",
    "        images = random.sample(images, n_selected)\n",
    "    for image_path in images:\n",
    "        assert os.path.isfile(image_path)\n",
    "        init_image = load_img(image_path, [512, 512]).to(device)\n",
    "        ref_image = load_img(image_path, [224, 224]).to(device)\n",
    "        init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
    "        ref_image = repeat(ref_image, '1 ... -> b ...', b=batch_size)\n",
    "        # sampling image\n",
    "        sampled_imgs = random.sample(files, n_perturb_samples)\n",
    "        perturbed_codes = []\n",
    "        logits, ocodes, feature, feature_dist, feature_euc = model.get_learned_conditioning(ref_image)\n",
    "        perturbed_codes.append(feature_euc)\n",
    "        # load sampled images\n",
    "        if n_perturb_samples > 0:\n",
    "            sampled_images = []\n",
    "            for sampled_img in sampled_imgs:\n",
    "                assert os.path.isfile(sampled_img)\n",
    "                sampled_image = load_img(sampled_img, [224, 224]).to(device)\n",
    "                sampled_images.append(sampled_image)\n",
    "        \n",
    "            _, hyp_code = get_hyp_codes(model, ref_image[0].unsqueeze(0))\n",
    "            _, perturb_codes = get_hyp_codes(model, sampled_images)\n",
    "            distances = [5.6]\n",
    "            num_samples = len(sampled_images)\n",
    "            feature_geo, feature_geo_normalized, feature_geo_current_target_boundaries, dist_to_start, perturbation_distances = geo_perturbation(\n",
    "                hyp_code, distances, target_radius=6.2126, perturb_codes=perturb_codes, num_samples=num_samples)\n",
    "            for i in feature_geo_current_target_boundaries:\n",
    "                feature = repeat(i, '1 ... -> b ...', b=batch_size)\n",
    "                feature_euc = get_condition_given_hyp_codes(model, feature)\n",
    "                perturbed_codes.append(feature_euc)\n",
    "        init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
    "        t_enc = int(strength * ddim_steps)\n",
    "\n",
    "        uc = get_unconditional_embedding(model, scale, n_samples, device, ref_image)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with precision_scope(\"cuda\"):\n",
    "                with model.ema_scope():\n",
    "                    tic = time.time()\n",
    "                    all_samples = list()\n",
    "                    shape = [C, 64, 64]\n",
    "                    # encode (scaled latent)\n",
    "                    if strength < 1.0:\n",
    "                        z_enc = sampler.stochastic_encode(\n",
    "                            init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
    "                    # print(f\"z_enc shape: {z_enc.shape}\")\n",
    "                    else:\n",
    "                        z_enc = torch.randn([n_samples, 4, 64, 64], device=device)\n",
    "                    # decode it\n",
    "                    i = 0\n",
    "                    # for c in rescaled_codes:\n",
    "                    # for c in interpolated_codes:\n",
    "                    # for c in reconstruct_codes:\n",
    "                    for c in perturbed_codes:\n",
    "                    # for c in compare_codes:\n",
    "                    # for c in fused_codes:\n",
    "                    # for c in code:\n",
    "                        if strength == 1.0:\n",
    "                            samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
    "                                                            conditioning=c,\n",
    "                                                            batch_size=n_samples,\n",
    "                                                            shape=shape,\n",
    "                                                            verbose=False,\n",
    "                                                            unconditional_guidance_scale=scale,\n",
    "                                                            unconditional_conditioning=uc,\n",
    "                                                            eta=ddim_eta,\n",
    "                                                            x_T=z_enc)\n",
    "                        else:\n",
    "                            samples_ddim = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=scale,\n",
    "                                                        unconditional_conditioning=uc)\n",
    "\n",
    "                        x_samples = model.decode_first_stage(samples_ddim)\n",
    "                        x_samples = torch.clamp(\n",
    "                            (x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "                        \n",
    "                        if not skip_save:\n",
    "                            for x_sample in x_samples:\n",
    "                                x_sample = 255. * \\\n",
    "                                    rearrange(x_sample.cpu().numpy(),\n",
    "                                            'c h w -> h w c')\n",
    "                                output_dir = os.path.join(outdir, image_path.split('/')[-2])\n",
    "                                if not os.path.exists(output_dir):\n",
    "                                    os.makedirs(output_dir)\n",
    "                                Image.fromarray(x_sample.astype(np.uint8)).save(\n",
    "                                    os.path.join(output_dir, str(i) + '_' + image_path.split('/')[-1]))\n",
    "                                i += 1\n",
    "\n",
    "            toc = time.time()\n",
    "            count += 1\n",
    "            print('finish {}/{}'.format(count, len(images)))\n",
    "    break\n",
    "print(f\"Your samples are ready and waiting for you \\n\"\n",
    "      f\" \\nEnjoy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2040 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2040/2040 [00:10<00:00, 191.55it/s]\n"
     ]
    }
   ],
   "source": [
    "copy_dataset('/data2/mhf/DXL/Lingxiao/Codes/HypDiffusion/outputs/flowers_eva_random_genrated_cfg_1.7_strength_0.95_r_5.5_10_samples_12_each',\n",
    "             '/data2/mhf/DXL/Lingxiao/Codes/HypDiffusion/outputs/flowers_eva_random_genrated_cfg_1.7_strength_0.95_r_5.5_10_samples_12_each_fid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1050/1050 [00:01<00:00, 915.03it/s]\n"
     ]
    }
   ],
   "source": [
    "copy_dataset('/data2/mhf/DXL/Lingxiao/datasets/animals_eva/test_30/test',\n",
    "             '/data2/mhf/DXL/Lingxiao/datasets/animals_eva/test_30/test_fid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_redundant_images(path_1, path_2, dataset_type):\n",
    "    classes, _ = find_classes(path_1)\n",
    "    for class_name in tqdm(classes):\n",
    "        class_path = os.path.join(path_1, class_name)\n",
    "        images = make_dataset(class_path)\n",
    "        images_2 = make_dataset(os.path.join(path_2, class_name))\n",
    "        # print(len(images))\n",
    "        image_names = []\n",
    "        for image in images:\n",
    "            if dataset_type == 'flowers':\n",
    "                image_name = image.split('/')[-1].split('_')[-2] + '_' + image.split('/')[-1].split('_')[-1]\n",
    "            elif dataset_type == 'nabirds':\n",
    "                image_name = image.split('/')[-1][2:]\n",
    "            elif dataset_type == 'animals':\n",
    "                image_name = image.split('/')[-1][2:]\n",
    "            elif dataset_type == 'vggfaces':\n",
    "                image_name = image.split('/')[-1][2:]\n",
    "            \n",
    "            # print(image_name)\n",
    "            image_names.append(image_name)\n",
    "        for image in images_2:\n",
    "            if dataset_type == 'flowers':\n",
    "                image_name = image.split('/')[-1].split('_')[-2] + '_' + image.split('/')[-1].split('_')[-1]\n",
    "            elif dataset_type == 'nabirds':\n",
    "                image_name = image.split('/')[-1]\n",
    "            elif dataset_type == 'animals':\n",
    "                image_name = image.split('/')[-1]\n",
    "            elif dataset_type == 'vggfaces':\n",
    "                image_name = image.split('/')[-1]\n",
    "            if image_name not in image_names:\n",
    "\n",
    "                img = cv2.imread(image)\n",
    "                new_path = os.path.join(path_2+'_clear', class_name)\n",
    "                if not os.path.exists(new_path):\n",
    "                    os.makedirs(new_path)\n",
    "                if img is not None:\n",
    "                    cv2.imwrite(new_path + '/' + image_name, img)\n",
    "                    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 572/572 [00:16<00:00, 35.64it/s]\n"
     ]
    }
   ],
   "source": [
    "remove_redundant_images(\n",
    "    '/data2/mhf/DXL/Lingxiao/Codes/HypDiffusion/outputs/vggfaces_eva_genrated_cfg_1.3_strength_0.95_r_5.4_30_samples_6_each_128',\n",
    "    '/data2/mhf/DXL/Lingxiao/datasets/vggfaces_eva/test', 'vggfaces')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anydoor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
