{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/mhf/DXL/Lingxiao/anaconda/envs/Paint-by-Example/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import PIL\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '/data2/mhf/DXL/Lingxiao/Codes/HypDiffusion')\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm, trange\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from itertools import islice\n",
    "from einops import rearrange, repeat\n",
    "from torch import autocast\n",
    "from contextlib import nullcontext\n",
    "from pytorch_lightning import seed_everything\n",
    "import cv2\n",
    "import time\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "from ldm.models.diffusion.ddim_org import DDIMSampler\n",
    "from ldm.models.diffusion.dpm_solver_org import DPMSolverSampler\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neccessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils.pmath import *\n",
    "# we utilize geoopt package for hyperbolic calculation\n",
    "import geoopt.manifolds.stereographic.math as gmath\n",
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "\n",
    "def load_model_from_config(config, ckpt, verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_img(path, size=[256, 256]):\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    w, h = image.shape[:2]\n",
    "    print(f\"loaded input image of size ({w}, {h}) from {path}\")\n",
    "    # resize to integer multiple of 32\n",
    "    # w, h = map(lambda x: x - x % 32, (w, h))\n",
    "    w, h = size\n",
    "    image = cv2.resize(image, (w, h), interpolation=cv2.INTER_LANCZOS4)\n",
    "    image = np.array(image).astype(np.uint8)\n",
    "\n",
    "    image = (image / 127.5 - 1.0).astype(np.float32)\n",
    "    image = image[None].transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "def load_model_and_get_prompt_embedding(model, scale, n_samples, device, prompts, inv=False):\n",
    "\n",
    "    if inv:\n",
    "        inv_emb = model.get_learned_conditioning(prompts, inv)\n",
    "        c = uc = inv_emb\n",
    "    else:\n",
    "        inv_emb = None\n",
    "\n",
    "    if scale != 1.0:\n",
    "        uc = model.get_learned_conditioning(\n",
    "            n_samples * [torch.zeros((1, 3, 224, 224))])\n",
    "    else:\n",
    "        uc = None\n",
    "    c = model.get_learned_conditioning(prompts)\n",
    "\n",
    "    return c, uc, inv_emb\n",
    "\n",
    "def get_hyp_codes(model, prompts):\n",
    "    # return latent codes in the hyperbolic space for the given prompts\n",
    "    logits, feature_dist, feature_euc = model.get_learned_conditioning(prompts)\n",
    "    return feature_dist\n",
    "\n",
    "def get_condition_given_hyp_codes(model, hyp_codes):\n",
    "    # return latent codes in the CLIP space for the given latent codes in hyperbolic space\n",
    "    logits, feature_dist, feature_euc = model.get_learned_conditioning(hyp_codes, input_feature=True)\n",
    "    return feature_euc\n",
    "\n",
    "\n",
    "# rescale function\n",
    "\n",
    "\n",
    "def rescale(target_radius, x):\n",
    "    r_change = target_radius / \\\n",
    "        dist0(gmath.mobius_scalar_mul(\n",
    "            r=torch.tensor(1), x=x, k=torch.tensor(-1.0)))\n",
    "    return gmath.mobius_scalar_mul(r=r_change, x=x, k=torch.tensor(-1.0))\n",
    "\n",
    "# function for generating images with fixed radius (also contains raw geodesic images of 'shorten' images, and stretched images to boundary)\n",
    "\n",
    "\n",
    "def geo_interpolate_fix_r(x, y, interval, target_radius, save_codes=False):\n",
    "    feature_geo = []\n",
    "    feature_geo_normalized = []\n",
    "    images_to_plot_raw_geo = []\n",
    "    images_to_plot_target_radius = []\n",
    "    images_to_plot_boundary = []\n",
    "    dist_to_start = []\n",
    "    target_radius_ratio = torch.tensor(target_radius/6.2126)\n",
    "    geodesic_start_short = gmath.mobius_scalar_mul(\n",
    "        r=target_radius_ratio, x=x, k=torch.tensor(-1.0))\n",
    "    geodesic_end_short = gmath.mobius_scalar_mul(\n",
    "        r=target_radius_ratio, x=y, k=torch.tensor(-1.0))\n",
    "    index = 0\n",
    "    for i in interval:\n",
    "        # this is raw image on geodesic, instead of fixed radius\n",
    "        feature_geo_current = gmath.geodesic(t=torch.tensor(\n",
    "            i), x=geodesic_start_short, y=geodesic_end_short, k=torch.tensor(-1.0))\n",
    "\n",
    "        # here we fix the radius and don't revert them now\n",
    "        r_change = target_radius / \\\n",
    "            dist0(gmath.mobius_scalar_mul(r=torch.tensor(1),\n",
    "                  x=feature_geo_current, k=torch.tensor(-1.0)))\n",
    "        feature_geo.append(feature_geo_current)\n",
    "        feature_geo_current_target_radius = gmath.mobius_scalar_mul(\n",
    "            r=r_change, x=feature_geo_current, k=torch.tensor(-1.0))\n",
    "        feature_geo_normalized.append(feature_geo_current_target_radius)\n",
    "        dist = gmath.dist(\n",
    "            geodesic_start_short, feature_geo_current_target_radius, k=torch.tensor(-1.0))\n",
    "        dist_to_start.append(dist)\n",
    "\n",
    "        # here is to revert the feature to boundary\n",
    "        r_change_to_boundary = 6.2126 / \\\n",
    "            dist0(gmath.mobius_scalar_mul(r=torch.tensor(1),\n",
    "                  x=feature_geo_current, k=torch.tensor(-1.0)))\n",
    "        feature_geo_current_target_boundary = gmath.mobius_scalar_mul(\n",
    "            r=r_change_to_boundary, x=feature_geo_current, k=torch.tensor(-1.0))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_raw_geo, _, _, _, _ = net.forward(x=feature_geo_current.unsqueeze(\n",
    "                0), codes=None, batch_size=1, input_feature=True, input_code=False)\n",
    "            image, _, _, codes_to_save, _ = net.forward(x=feature_geo_current_target_radius.unsqueeze(\n",
    "                0), codes=None, batch_size=1, input_feature=True, input_code=False)\n",
    "            if save_codes:\n",
    "                torch.save(codes_to_save, os.path.join(\n",
    "                    '/data2/mhf/DXL/Lingxiao/Codes/hae_editing/save_latent_codes', str(i)+'_'+str(index)+'.pth'))\n",
    "            index += 1\n",
    "            image_boundary, _, _, _, _ = net.forward(x=feature_geo_current_target_boundary.unsqueeze(\n",
    "                0), codes=None, batch_size=1, input_feature=True, input_code=False)\n",
    "        images_to_plot_raw_geo.append(image_raw_geo)\n",
    "        images_to_plot_target_radius.append(image)\n",
    "        images_to_plot_boundary.append(image_boundary)\n",
    "\n",
    "    return images_to_plot_raw_geo, images_to_plot_target_radius, images_to_plot_boundary, dist_to_start\n",
    "\n",
    "# function for generating images with fixed radius with optional latent codes list output\n",
    "\n",
    "\n",
    "def geo_interpolate_fix_r_with_codes(x, y, interval, target_radius):\n",
    "    # please use this with batch_size = 1\n",
    "    feature_geo = []\n",
    "    feature_geo_normalized = []\n",
    "    images_to_plot_raw_geo = []\n",
    "    images_to_plot_target_radius = []\n",
    "    images_to_plot_boundary = []\n",
    "    dist_to_start = []\n",
    "    target_radius_ratio = torch.tensor(target_radius/6.2126)\n",
    "    geodesic_start_short = gmath.mobius_scalar_mul(\n",
    "        r=target_radius_ratio, x=x, k=torch.tensor(-1.0))\n",
    "    geodesic_end_short = gmath.mobius_scalar_mul(\n",
    "        r=target_radius_ratio, x=y, k=torch.tensor(-1.0))\n",
    "    for i in interval:\n",
    "        # this is raw image on geodesic, instead of fixed radius\n",
    "        feature_geo_current = gmath.geodesic(t=torch.tensor(\n",
    "            i), x=geodesic_start_short, y=geodesic_end_short, k=torch.tensor(-1.0))\n",
    "\n",
    "        # here we fix the radius and don't revert them now\n",
    "        r_change = target_radius / \\\n",
    "            dist0(gmath.mobius_scalar_mul(r=torch.tensor(1),\n",
    "                  x=feature_geo_current, k=torch.tensor(-1.0)))\n",
    "        feature_geo.append(feature_geo_current)\n",
    "        feature_geo_current_target_radius = gmath.mobius_scalar_mul(\n",
    "            r=r_change, x=feature_geo_current, k=torch.tensor(-1.0))\n",
    "        feature_geo_normalized.append(feature_geo_current_target_radius)\n",
    "        dist = gmath.dist(\n",
    "            geodesic_start_short, feature_geo_current_target_radius, k=torch.tensor(-1.0))\n",
    "        dist_to_start.append(dist)\n",
    "        # print(feature_geo_current_target_radius.norm())\n",
    "\n",
    "        # here is to revert the feature to boundary\n",
    "        r_change_to_boundary = 6.2126 / \\\n",
    "            dist0(gmath.mobius_scalar_mul(r=torch.tensor(1),\n",
    "                  x=feature_geo_current, k=torch.tensor(-1.0)))\n",
    "        feature_geo_current_target_boundary = gmath.mobius_scalar_mul(\n",
    "            r=r_change_to_boundary, x=feature_geo_current, k=torch.tensor(-1.0))\n",
    "        # print(feature_geo_current_target_boundary.norm())\n",
    "\n",
    "        # now codes do not affect outputs\n",
    "        with torch.no_grad():\n",
    "            image_raw_geo, _, _, _, _ = net.forward(x=feature_geo_current.unsqueeze(\n",
    "                0), codes=None, batch_size=1, input_feature=True, input_code=False)\n",
    "            image, _, _, codes_target_radius, _ = net.forward(x=feature_geo_current_target_radius.unsqueeze(\n",
    "                0), codes=None, batch_size=1, input_feature=True, input_code=False)\n",
    "            image_boundary, _, _, codes_boundary, _ = net.forward(x=feature_geo_current_target_boundary.unsqueeze(\n",
    "                0), codes=None, batch_size=1, input_feature=True, input_code=False)\n",
    "        images_to_plot_raw_geo.append(image_raw_geo)\n",
    "        images_to_plot_target_radius.append(image)\n",
    "        images_to_plot_boundary.append(image_boundary)\n",
    "\n",
    "    return images_to_plot_raw_geo, images_to_plot_target_radius, images_to_plot_boundary, dist_to_start, [codes_target_radius, codes_boundary, feature_geo_current_target_radius, feature_geo_current_target_boundary]\n",
    "\n",
    "# the function defines easy perturbation on any given radius\n",
    "\n",
    "\n",
    "def generate_perturbation_r_with_raw_inv(x, target_radius, interval, seed, size, save_codes=False):\n",
    "    # 3 arguments, raw image feature, target radius and interval(actually the ratio).\n",
    "    images_perturbed = []\n",
    "    dist_perturbed = []\n",
    "    torch.manual_seed(seed=seed)\n",
    "    perturb = torch.rand(6, 512).cuda()\n",
    "    for i in range(size):\n",
    "        target_rad_perturb = 6.2126\n",
    "        ratio = target_rad_perturb/dist0(perturb[i])\n",
    "        perturb_current = gmath.mobius_scalar_mul(\n",
    "            r=ratio, x=perturb[i], k=torch.tensor(-1.0))\n",
    "        _, images_to_plot_target_radius, _, dist_to_start = geo_interpolate_fix_r(\n",
    "            x=x, y=perturb_current, interval=interval, target_radius=target_radius, save_codes=save_codes)\n",
    "        print(dist_to_start)\n",
    "        dist_perturbed.append(dist_to_start[0])\n",
    "        images_perturbed.append(images_to_plot_target_radius[0])\n",
    "\n",
    "    raw_image, _, _, _ = geo_interpolate_fix_r(x=x, y=perturb_current, interval=[\n",
    "                                               0], target_radius=target_radius)\n",
    "    images_perturbed.insert(0, raw_image[0])\n",
    "    return images_perturbed, dist_perturbed\n",
    "\n",
    "# this further function allows using specific target image as perturbation\n",
    "\n",
    "\n",
    "def generate_perturbation_r_with_raw_inv_pick(x, y, target_radius, interval, seed, save_codes=False):\n",
    "    # 3 arguments, raw image feature, target radius and interval(actually the ratio).\n",
    "    images_perturbed = []\n",
    "    dist_perturbed = []\n",
    "    torch.manual_seed(seed=seed)\n",
    "    # perturb = torch.rand(6,512).cuda()\n",
    "    perturb = y\n",
    "    for i in range(len(y)):\n",
    "        target_rad_perturb = 6.2126\n",
    "        ratio = target_rad_perturb/dist0(perturb[i])\n",
    "        perturb_current = gmath.mobius_scalar_mul(\n",
    "            r=ratio, x=perturb[i], k=torch.tensor(-1.0))\n",
    "\n",
    "        if False:\n",
    "            with torch.no_grad():\n",
    "                image, _, _, _, _ = net.forward(x=perturb_current.unsqueeze(\n",
    "                    0), codes=None, batch_size=1, input_feature=True, input_code=False)\n",
    "\n",
    "            fig = plt.figure(figsize=(5, 5))\n",
    "            gs = fig.add_gridspec(1, 1)\n",
    "            for i in range(1):\n",
    "                if i == 0:\n",
    "                    fig.add_subplot(gs[0, i])\n",
    "                    plt.axis('off')\n",
    "                    plt.title(f'perturb = {target_rad_perturb}')\n",
    "                    plt.imshow(tensor2im(image.squeeze(0)))\n",
    "\n",
    "        # interval = [0.42]\n",
    "        _, images_to_plot_target_radius, _, dist_to_start = geo_interpolate_fix_r(\n",
    "            x=x, y=perturb_current, interval=interval, target_radius=target_radius, save_codes=save_codes)\n",
    "        print(dist_to_start)\n",
    "        dist_perturbed.append(dist_to_start[0])\n",
    "        images_perturbed.append(images_to_plot_target_radius[0])\n",
    "\n",
    "    raw_image, _, _, _ = geo_interpolate_fix_r(x=x, y=perturb_current, interval=[\n",
    "                                               0], target_radius=target_radius, save_codes=save_codes)\n",
    "    images_perturbed.insert(0, raw_image[0])\n",
    "    return images_perturbed, dist_perturbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables\n",
    "init_image_path = './inputs/same_domain_test/123/test3.jpg'\n",
    "ref_image_path = './inputs/same_domain_test/123/test3.jpg'\n",
    "prompt = 'a image of a golden retriever'\n",
    "outdir = './outputs'\n",
    "skip_grid = False\n",
    "skip_save = True\n",
    "ddim_steps = 50\n",
    "ddim_eta = 0.0\n",
    "n_iter = 1\n",
    "C = 4\n",
    "f = 8\n",
    "n_samples = 20\n",
    "n_rows = 0\n",
    "scale = 7.5\n",
    "strength = 0.95\n",
    "config_path = './configs/stable-diffusion/v2_inference_text.yaml'\n",
    "ckpt = '/data2/mhf/DXL/Lingxiao/Codes/Paint-by-Example-test/models/Paint-by-Example/animal_faces/2024-09-11T11-30-12_v1/checkpoints/epoch=000035.ckpt'\n",
    "seed = 3408\n",
    "precision = 'autocast'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 3408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /data2/mhf/DXL/Lingxiao/Codes/Paint-by-Example-test/models/Paint-by-Example/animal_faces/2024-09-11T11-30-12_v1/checkpoints/epoch=000035.ckpt\n",
      "Global Step: 77724\n",
      "No module 'xformers'. Proceeding without it.\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 865.91 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data2/mhf/DXL/Lingxiao/Cache/huggingface/hub/models--openai--clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'logit_scale', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'visual_projection.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'text_projection.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use hyperbolic: True\n",
      "Loading HAE from checkpoint: /data2/mhf/DXL/Lingxiao/Codes/hyperediting/exp_out/hyper_styleGANinversion_animalfaces_512_5_30_init_v2/checkpoints/iteration_11000.pt\n",
      "Successfully loaded model!\n"
     ]
    }
   ],
   "source": [
    "seed_everything(seed)\n",
    "\n",
    "config = OmegaConf.load(f\"{config_path}\")\n",
    "model = load_model_from_config(config, f\"{ckpt}\")\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "print(\"Successfully loaded model!\")\n",
    "\n",
    "sampler = DDIMSampler(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(outdir, exist_ok=True)\n",
    "outpath = outdir\n",
    "\n",
    "batch_size = n_samples\n",
    "n_rows = n_rows if n_rows > 0 else batch_size\n",
    "\n",
    "sample_path = os.path.join(outpath, \"samples\")\n",
    "os.makedirs(sample_path, exist_ok=True)\n",
    "base_count = len(os.listdir(sample_path))\n",
    "grid_count = len(os.listdir(outpath)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded input image of size (87, 103) from ./inputs/same_domain_test/123/test3.jpg\n",
      "loaded input image of size (87, 103) from ./inputs/same_domain_test/123/test3.jpg\n",
      "loaded input image of size (87, 103) from ./inputs/same_domain_test/123/test3.jpg\n",
      "target t_enc is 47 steps\n"
     ]
    }
   ],
   "source": [
    "# load prompt\n",
    "data = []\n",
    "for i in range(batch_size):\n",
    "    data.append([prompt])\n",
    "\n",
    "# load images\n",
    "# load init image\n",
    "assert os.path.isfile(init_image_path)\n",
    "init_image = load_img(init_image_path, [512, 512]).to(device)\n",
    "init_image_resized = load_img(init_image_path, [224, 224]).to(device)\n",
    "init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
    "init_image_resized = repeat(init_image_resized, '1 ... -> b ...', b=batch_size)\n",
    "init_latent = model.get_first_stage_encoding(\n",
    "    model.encode_first_stage(init_image))  # move to latent space\n",
    "\n",
    "# load ref image\n",
    "assert os.path.isfile(ref_image_path)\n",
    "ref_image = load_img(ref_image_path, [224, 224]).to(device)\n",
    "ref_image = repeat(ref_image, '1 ... -> b ...', b=batch_size)\n",
    "\n",
    "sampler.make_schedule(ddim_num_steps=ddim_steps,\n",
    "                        ddim_eta=ddim_eta, verbose=False)\n",
    "\n",
    "assert 0. <= strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
    "t_enc = int(strength * ddim_steps)\n",
    "print(f\"target t_enc is {t_enc} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulate latent codes in Hyperbolic space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a image of a golden retriever'], ['a image of a golden retriever'], ['a image of a golden retriever'], ['a image of a golden retriever'], ['a image of a golden retriever'], ['a image of a golden retriever'], ['a image of a golden retriever'], ['a image of a golden retriever'], ['a image of a golden retriever'], ['a image of a golden retriever'], ['a image of a golden retriever'], ['a image of a golden retriever'], ['a image of a golden retriever'], ['a image of a golden retriever'], ['a image of a golden retriever'], ['a image of a golden retriever'], ['a image of a golden retriever'], ['a image of a golden retriever'], ['a image of a golden retriever'], ['a image of a golden retriever']]\n",
      "torch.Size([1, 77, 768])\n",
      "torch.Size([1, 768])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given normalized_shape=[1024], expected input with shape [*, 1024], but got input of size[1, 1, 1, 768]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(data)\n\u001b[0;32m----> 2\u001b[0m hyp_code \u001b[38;5;241m=\u001b[39m \u001b[43mget_hyp_codes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(hyp_code\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(gmath\u001b[38;5;241m.\u001b[39mdist0(hyp_code, k\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m)))\n",
      "Cell \u001b[0;32mIn[2], line 65\u001b[0m, in \u001b[0;36mget_hyp_codes\u001b[0;34m(model, prompts)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_hyp_codes\u001b[39m(model, prompts):\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# return latent codes in the hyperbolic space for the given prompts\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     logits, feature_dist, feature_euc \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_learned_conditioning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m feature_dist\n",
      "File \u001b[0;32m/data2/mhf/DXL/Lingxiao/Codes/HypDiffusion/ldm/models/diffusion/ddpm.py:666\u001b[0m, in \u001b[0;36mLatentDiffusion.get_learned_conditioning\u001b[0;34m(self, c, inv, input_feature, sample_mode)\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcond_stage_forward \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcond_stage_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencode\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcond_stage_model\u001b[38;5;241m.\u001b[39mencode):\n\u001b[0;32m--> 666\u001b[0m         c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcond_stage_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    667\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, DiagonalGaussianDistribution):\n\u001b[1;32m    668\u001b[0m             c \u001b[38;5;241m=\u001b[39m c\u001b[38;5;241m.\u001b[39mmode()\n",
      "File \u001b[0;32m/data2/mhf/DXL/Lingxiao/Codes/HypDiffusion/ldm/modules/encoders/modules.py:580\u001b[0m, in \u001b[0;36mHyperbolicCLIPTextEmbedder.encode\u001b[0;34m(self, image, inv, input_feature, sample_mode, device)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data2/mhf/DXL/Lingxiao/anaconda/envs/Paint-by-Example/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data2/mhf/DXL/Lingxiao/Codes/HypDiffusion/ldm/modules/encoders/modules.py:538\u001b[0m, in \u001b[0;36mHyperbolicCLIPTextEmbedder.forward\u001b[0;34m(self, x, y, batch_size, resize, latent_mask, input_code, sample_mode, inject_latent, return_latents, alpha, input_feature, inv)\u001b[0m\n\u001b[1;32m    536\u001b[0m     codes \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 538\u001b[0m     codes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    540\u001b[0m ocodes \u001b[38;5;241m=\u001b[39m codes\n\u001b[1;32m    541\u001b[0m feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_encoder(ocodes)\n",
      "File \u001b[0;32m/data2/mhf/DXL/Lingxiao/Codes/HypDiffusion/ldm/modules/encoders/modules.py:409\u001b[0m, in \u001b[0;36mFrozenCLIPTextEmbedder.encode\u001b[0;34m(self, text, inv, device)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device\n\u001b[0;32m--> 409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minv\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data2/mhf/DXL/Lingxiao/anaconda/envs/Paint-by-Example/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data2/mhf/DXL/Lingxiao/Codes/HypDiffusion/ldm/modules/encoders/modules.py:402\u001b[0m, in \u001b[0;36mFrozenCLIPTextEmbedder.forward\u001b[0;34m(self, text, inv)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mpooler_output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    401\u001b[0m z \u001b[38;5;241m=\u001b[39m z\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 402\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_ln(z)\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m z\n",
      "File \u001b[0;32m/data2/mhf/DXL/Lingxiao/anaconda/envs/Paint-by-Example/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data2/mhf/DXL/Lingxiao/Codes/HypDiffusion/ldm/modules/encoders/xf.py:129\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: th\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresblocks:\n\u001b[0;32m--> 129\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/data2/mhf/DXL/Lingxiao/anaconda/envs/Paint-by-Example/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data2/mhf/DXL/Lingxiao/Codes/HypDiffusion/ldm/modules/encoders/xf.py:99\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: th\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 99\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    100\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(x))\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/data2/mhf/DXL/Lingxiao/anaconda/envs/Paint-by-Example/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data2/mhf/DXL/Lingxiao/Codes/HypDiffusion/ldm/modules/encoders/xf.py:28\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: th\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m/data2/mhf/DXL/Lingxiao/anaconda/envs/Paint-by-Example/lib/python3.8/site-packages/torch/nn/modules/normalization.py:189\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data2/mhf/DXL/Lingxiao/anaconda/envs/Paint-by-Example/lib/python3.8/site-packages/torch/nn/functional.py:2486\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2484\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2485\u001b[0m     )\n\u001b[0;32m-> 2486\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given normalized_shape=[1024], expected input with shape [*, 1024], but got input of size[1, 1, 1, 768]"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "hyp_code = get_hyp_codes(model, data[0])\n",
    "print(hyp_code.shape)\n",
    "print(gmath.dist0(hyp_code, k=torch.tensor(-1.0)))\n",
    "# this is used for generating figure of varying radius in our paper\n",
    "rescaled_codes = []\n",
    "target_radii = [6.2126, 4, 2.5, 1, 0.5, 0]\n",
    "for i in target_radii:\n",
    "    hyp_code_rescaled = rescale(i, hyp_code)\n",
    "    hyp_code_rescaled = repeat(hyp_code_rescaled, '1 ... -> b ...', b=batch_size)\n",
    "    feature_euc = get_condition_given_hyp_codes(model, hyp_code_rescaled)\n",
    "    rescaled_codes.append(feature_euc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rescaled_codes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m     z_enc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn([n_samples, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m], device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# decode it\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[43mrescaled_codes\u001b[49m:\n\u001b[1;32m     16\u001b[0m     samples_ddim, _ \u001b[38;5;241m=\u001b[39m sampler\u001b[38;5;241m.\u001b[39msample(S\u001b[38;5;241m=\u001b[39mddim_steps,\n\u001b[1;32m     17\u001b[0m                                         conditioning\u001b[38;5;241m=\u001b[39mc,\n\u001b[1;32m     18\u001b[0m                                         batch_size\u001b[38;5;241m=\u001b[39mn_samples,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m                                         eta\u001b[38;5;241m=\u001b[39mddim_eta,\n\u001b[1;32m     24\u001b[0m                                         x_T\u001b[38;5;241m=\u001b[39mz_enc)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    samples_ddim = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=scale,\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m                                unconditional_conditioning=uc,)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m                                '''\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rescaled_codes' is not defined"
     ]
    }
   ],
   "source": [
    "precision_scope = autocast if precision == \"autocast\" else nullcontext\n",
    "with torch.no_grad():\n",
    "    with precision_scope(\"cuda\"):\n",
    "        with model.ema_scope():\n",
    "            tic = time.time()\n",
    "            all_samples = list()\n",
    "            shape = [C, 64, 64]\n",
    "            # encode (scaled latent)\n",
    "            if strength < 1.0:\n",
    "                z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
    "            # print(f\"z_enc shape: {z_enc.shape}\")\n",
    "            else:\n",
    "                z_enc = torch.randn([n_samples, 4, 64, 64], device=device)\n",
    "            # decode it\n",
    "            for c in rescaled_codes:\n",
    "                samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
    "                                                    conditioning=c,\n",
    "                                                    batch_size=n_samples,\n",
    "                                                    shape=shape,\n",
    "                                                    verbose=False,\n",
    "                                                    unconditional_guidance_scale=scale,\n",
    "                                                    unconditional_conditioning=c,\n",
    "                                                    eta=ddim_eta,\n",
    "                                                    x_T=z_enc)\n",
    "                '''\n",
    "                samples_ddim = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=scale,\n",
    "                                            unconditional_conditioning=uc,)\n",
    "                                            '''\n",
    "\n",
    "                x_samples = model.decode_first_stage(samples_ddim)\n",
    "                x_samples = torch.clamp(\n",
    "                    (x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "                if not skip_save:\n",
    "                    for x_sample in x_samples:\n",
    "                        x_sample = 255. * \\\n",
    "                            rearrange(x_sample.cpu().numpy(),\n",
    "                                        'c h w -> h w c')\n",
    "                        Image.fromarray(x_sample.astype(np.uint8)).save(\n",
    "                            os.path.join(sample_path, f\"{base_count:05}.png\"))\n",
    "                        base_count += 1\n",
    "                all_samples.append(x_samples)\n",
    "\n",
    "                if not skip_grid:\n",
    "                    # additionally, save as grid\n",
    "                    grid = torch.stack(all_samples, 0)\n",
    "                    grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "                    grid = make_grid(grid, nrow=n_rows)\n",
    "\n",
    "            # to image\n",
    "            grid = 255. * \\\n",
    "                rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "            Image.fromarray(grid.astype(np.uint8)).save(\n",
    "                os.path.join(outpath, f'grid-{grid_count:04}.png'))\n",
    "            grid_count += 1\n",
    "            del grid\n",
    "\n",
    "        toc = time.time()\n",
    "\n",
    "print(f\"Your samples are ready and waiting for you here: \\n{outpath} \\n\"\n",
    "        f\" \\nEnjoy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean of the variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (20, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|| 50/50 [00:32<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 512, 512])\n",
      "Data shape for DDIM sampling is (20, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|| 50/50 [00:32<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 512, 512])\n",
      "Data shape for DDIM sampling is (20, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|| 50/50 [00:33<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 512, 512])\n",
      "Data shape for DDIM sampling is (20, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|| 50/50 [00:33<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 512, 512])\n",
      "Data shape for DDIM sampling is (20, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|| 50/50 [00:33<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 512, 512])\n",
      "Data shape for DDIM sampling is (20, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|| 50/50 [00:33<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 512, 512])\n",
      "Your samples are ready and waiting for you here: \n",
      "./outputs \n",
      " \n",
      "Enjoy.\n"
     ]
    }
   ],
   "source": [
    "precision_scope = autocast if precision == \"autocast\" else nullcontext\n",
    "with torch.no_grad():\n",
    "    with precision_scope(\"cuda\"):\n",
    "        with model.ema_scope():\n",
    "            tic = time.time()\n",
    "            all_samples = list()\n",
    "            shape = [C, 64, 64]\n",
    "            # encode (scaled latent)\n",
    "            if strength < 1.0:\n",
    "                z_enc = sampler.stochastic_encode(\n",
    "                    init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
    "            # print(f\"z_enc shape: {z_enc.shape}\")\n",
    "            else:\n",
    "                z_enc = torch.randn([n_samples, 4, 64, 64], device=device)\n",
    "            # decode it\n",
    "            for c in rescaled_codes:\n",
    "                samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
    "                                                 conditioning=c,\n",
    "                                                 batch_size=n_samples,\n",
    "                                                 shape=shape,\n",
    "                                                 verbose=False,\n",
    "                                                 unconditional_guidance_scale=scale,\n",
    "                                                 unconditional_conditioning=c,\n",
    "                                                 eta=ddim_eta,\n",
    "                                                 x_T=z_enc)\n",
    "                '''\n",
    "                samples_ddim = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=scale,\n",
    "                                            unconditional_conditioning=uc,)\n",
    "                                            '''\n",
    "\n",
    "                x_samples = model.decode_first_stage(samples_ddim)\n",
    "                x_samples = torch.clamp(\n",
    "                    (x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "                print(x_samples.shape)\n",
    "                x_samples_mean = x_samples.mean(0).unsqueeze(0)\n",
    "                if not skip_save:\n",
    "                    for x_sample in x_samples:\n",
    "                        x_sample = 255. * \\\n",
    "                            rearrange(x_sample.cpu().numpy(),\n",
    "                                      'c h w -> h w c')\n",
    "                        Image.fromarray(x_sample.astype(np.uint8)).save(\n",
    "                            os.path.join(sample_path, f\"{base_count:05}.png\"))\n",
    "                        base_count += 1\n",
    "                all_samples.append(x_samples_mean)\n",
    "\n",
    "                if not skip_grid:\n",
    "                    # additionally, save as grid\n",
    "                    grid = torch.stack(all_samples, 0)\n",
    "                    grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "                    grid = make_grid(grid, nrow=n_rows)\n",
    "\n",
    "            # to image\n",
    "            grid = 255. * \\\n",
    "                rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "            Image.fromarray(grid.astype(np.uint8)).save(\n",
    "                os.path.join(outpath, f'grid-{grid_count:04}.png'))\n",
    "            grid_count += 1\n",
    "            del grid\n",
    "\n",
    "        toc = time.time()\n",
    "\n",
    "print(f\"Your samples are ready and waiting for you here: \\n{outpath} \\n\"\n",
    "      f\" \\nEnjoy.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Paint-by-Example",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
