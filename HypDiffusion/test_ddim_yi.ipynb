{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/mhf/DXL/Lingxiao/anaconda/envs/Paint-by-Example/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import PIL\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '/data2/mhf/DXL/Lingxiao/Codes/HypDiffusion')\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm, trange\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from itertools import islice\n",
    "from einops import rearrange, repeat\n",
    "from torch import autocast\n",
    "from contextlib import nullcontext\n",
    "from pytorch_lightning import seed_everything\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "from ldm.models.diffusion.ddim_org import DDIMSampler\n",
    "from ldm.models.diffusion.dpm_solver_org import DPMSolverSampler\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neccessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils.pmath import *\n",
    "# we utilize geoopt package for hyperbolic calculation\n",
    "import geoopt.manifolds.stereographic.math as gmath\n",
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "\n",
    "def load_model_from_config(config, ckpt, verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_img(path, size=[256, 256]):\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    w, h = image.shape[:2]\n",
    "    print(f\"loaded input image of size ({w}, {h}) from {path}\")\n",
    "    # resize to integer multiple of 32\n",
    "    # w, h = map(lambda x: x - x % 32, (w, h))\n",
    "    w, h = size\n",
    "    image = cv2.resize(image, (w, h), interpolation=cv2.INTER_LANCZOS4)\n",
    "    image = np.array(image).astype(np.uint8)\n",
    "\n",
    "    image = (image / 127.5 - 1.0).astype(np.float32)\n",
    "    image = image[None].transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "def load_model_and_get_prompt_embedding(model, scale, n_samples, device, prompts, inv=False):\n",
    "\n",
    "    if inv:\n",
    "        inv_emb = model.get_learned_conditioning(prompts, inv)\n",
    "        c = uc = inv_emb\n",
    "    else:\n",
    "        inv_emb = None\n",
    "\n",
    "    if scale != 1.0:\n",
    "        uc = model.get_learned_conditioning(\n",
    "            n_samples * [torch.zeros((1, 3, 224, 224))])\n",
    "    else:\n",
    "        uc = None\n",
    "    c = model.get_learned_conditioning(prompts)\n",
    "\n",
    "    return c, uc, inv_emb\n",
    "\n",
    "def get_hyp_codes(model, prompts):\n",
    "    # return latent codes in the hyperbolic space for the given prompts\n",
    "    logits, feature, feature_dist, feature_euc = model.get_learned_conditioning(prompts)\n",
    "    return feature, feature_dist\n",
    "\n",
    "def get_hyp_codes_given_feature(model, feature):\n",
    "    # return latent codes in the hyperbolic space for the given latent codes in CLIP space\n",
    "    logits, feature, feature_dist, feature_euc = model.get_learned_conditioning(feature, input_feature=False, input_code=True)\n",
    "    return feature, feature_dist\n",
    "\n",
    "def get_condition_given_feature(model, feature):\n",
    "    # return latent codes in the CLIP space for the given latent codes in hyperbolic space\n",
    "    logits, feature, feature_dist, feature_euc = model.get_learned_conditioning(feature, input_feature=False, input_code=True)\n",
    "    return feature_euc\n",
    "\n",
    "def get_condition_given_hyp_codes(model, hyp_codes):\n",
    "    # return latent codes in the CLIP space for the given latent codes in hyperbolic space\n",
    "    logits, feature, feature_dist, feature_euc = model.get_learned_conditioning(hyp_codes, input_feature=True)\n",
    "    return feature_euc\n",
    "\n",
    "\n",
    "# rescale function\n",
    "def rescale(target_radius, x):\n",
    "    r_change = target_radius / \\\n",
    "        dist0(gmath.mobius_scalar_mul(\n",
    "            r=torch.tensor(1), x=x, k=torch.tensor(-1.0)))\n",
    "    return gmath.mobius_scalar_mul(r=r_change, x=x, k=torch.tensor(-1.0))\n",
    "\n",
    "\n",
    "# function for generating images with fixed radius (also contains raw geodesic images of 'shorten' images, and stretched images to boundary)\n",
    "def geo_interpolate_fix_r(x, y, interval, target_radius, save_codes=False):\n",
    "    feature_geo = []\n",
    "    feature_geo_normalized = []\n",
    "    dist_to_start = []\n",
    "    feature_geo_current_target_boundaries = []\n",
    "    target_radius_ratio = torch.tensor(target_radius/6.2126)\n",
    "    geodesic_start_short = gmath.mobius_scalar_mul(\n",
    "        r=target_radius_ratio, x=x, k=torch.tensor(-1.0))\n",
    "    geodesic_end_short = gmath.mobius_scalar_mul(\n",
    "        r=target_radius_ratio, x=y, k=torch.tensor(-1.0))\n",
    "    index = 0\n",
    "    for i in interval:\n",
    "        # this is raw image on geodesic, instead of fixed radius\n",
    "        feature_geo_current = gmath.geodesic(t=torch.tensor(\n",
    "            i), x=geodesic_start_short, y=geodesic_end_short, k=torch.tensor(-1.0))\n",
    "\n",
    "        # here we fix the radius and don't revert them now\n",
    "        r_change = target_radius / \\\n",
    "            dist0(gmath.mobius_scalar_mul(r=torch.tensor(1),\n",
    "                  x=feature_geo_current, k=torch.tensor(-1.0)))\n",
    "        feature_geo.append(feature_geo_current)\n",
    "        feature_geo_current_target_radius = gmath.mobius_scalar_mul(\n",
    "            r=r_change, x=feature_geo_current, k=torch.tensor(-1.0))\n",
    "        feature_geo_normalized.append(feature_geo_current_target_radius)\n",
    "        dist = gmath.dist(\n",
    "            geodesic_start_short, feature_geo_current_target_radius, k=torch.tensor(-1.0))\n",
    "        dist_to_start.append(dist)\n",
    "\n",
    "        # here is to revert the feature to boundary\n",
    "        r_change_to_boundary = 6.2126 / \\\n",
    "            dist0(gmath.mobius_scalar_mul(r=torch.tensor(1),\n",
    "                  x=feature_geo_current, k=torch.tensor(-1.0)))\n",
    "        feature_geo_current_target_boundary = gmath.mobius_scalar_mul(\n",
    "            r=r_change_to_boundary, x=feature_geo_current, k=torch.tensor(-1.0))\n",
    "        feature_geo_current_target_boundaries.append(feature_geo_current_target_boundary)\n",
    "\n",
    "    return feature_geo, feature_geo_normalized, feature_geo_current_target_boundaries, dist_to_start\n",
    "\n",
    "# function for generating images with fixed radius with optional latent codes list output\n",
    "\n",
    "\n",
    "def geo_interpolate_fix_r_with_codes(x, y, interval, target_radius):\n",
    "    # please use this with batch_size = 1\n",
    "    feature_geo = []\n",
    "    feature_geo_normalized = []\n",
    "    dist_to_start = []\n",
    "    target_radius_ratio = torch.tensor(target_radius/6.2126)\n",
    "    geodesic_start_short = gmath.mobius_scalar_mul(\n",
    "        r=target_radius_ratio, x=x, k=torch.tensor(-1.0))\n",
    "    geodesic_end_short = gmath.mobius_scalar_mul(\n",
    "        r=target_radius_ratio, x=y, k=torch.tensor(-1.0))\n",
    "    for i in interval:\n",
    "        # this is raw image on geodesic, instead of fixed radius\n",
    "        feature_geo_current = gmath.geodesic(t=torch.tensor(\n",
    "            i), x=geodesic_start_short, y=geodesic_end_short, k=torch.tensor(-1.0))\n",
    "\n",
    "        # here we fix the radius and don't revert them now\n",
    "        r_change = target_radius / \\\n",
    "            dist0(gmath.mobius_scalar_mul(r=torch.tensor(1),\n",
    "                  x=feature_geo_current, k=torch.tensor(-1.0)))\n",
    "        feature_geo.append(feature_geo_current)\n",
    "        feature_geo_current_target_radius = gmath.mobius_scalar_mul(\n",
    "            r=r_change, x=feature_geo_current, k=torch.tensor(-1.0))\n",
    "        feature_geo_normalized.append(feature_geo_current_target_radius)\n",
    "        dist = gmath.dist(\n",
    "            geodesic_start_short, feature_geo_current_target_radius, k=torch.tensor(-1.0))\n",
    "        dist_to_start.append(dist)\n",
    "        # print(feature_geo_current_target_radius.norm())\n",
    "\n",
    "        # here is to revert the feature to boundary\n",
    "        r_change_to_boundary = 6.2126 / \\\n",
    "            dist0(gmath.mobius_scalar_mul(r=torch.tensor(1),\n",
    "                  x=feature_geo_current, k=torch.tensor(-1.0)))\n",
    "        feature_geo_current_target_boundary = gmath.mobius_scalar_mul(\n",
    "            r=r_change_to_boundary, x=feature_geo_current, k=torch.tensor(-1.0))\n",
    "        # print(feature_geo_current_target_boundary.norm())\n",
    "\n",
    "    return dist_to_start, feature_geo_current, feature_geo_current_target_radius, feature_geo_current_target_boundary\n",
    "\n",
    "\n",
    "def geo_perturbation(x, distances, perturb_codes, target_radius=6.2126, num_samples=10, save_codes=False):\n",
    "    \"\"\"\n",
    "    该函数在双曲空间围绕给定的点 x 生成随机扰动，并确保扰动点的半径与 x 相同。\n",
    "    函数会为每个给定的双曲距离生成多个样本。\n",
    "\n",
    "    参数:\n",
    "        x (tensor): Poincaré圆盘中的起始点。\n",
    "        distances (list): 目标的双曲距离列表，例如 [0.01, 0.1, 0.2]。\n",
    "        target_radius (float): 要将扰动点缩放到的目标半径。\n",
    "        num_samples (int): 每个距离生成的样本数量。\n",
    "        save_codes (bool, 可选): 是否保存其他数据，默认为 False。\n",
    "    \n",
    "    返回:\n",
    "        tuple: 包含以下列表的元组：\n",
    "            - feature_geo: 未归一化的扰动点。\n",
    "            - feature_geo_normalized: 归一化到目标半径的扰动点。\n",
    "            - feature_geo_current_target_boundaries: 扰动点缩放到边界。\n",
    "            - dist_to_start: 扰动点与起始点 x 的双曲距离。\n",
    "            - perturbation_distances: 每个扰动点与起始点的双曲距离。\n",
    "    \"\"\"\n",
    "    \n",
    "    feature_geo = []\n",
    "    feature_geo_normalized = []\n",
    "    dist_to_start = []\n",
    "    feature_geo_current_target_boundaries = []\n",
    "    perturbation_distances = []  # 保存每个 feature_geo_current 和 geodesic_start_short 之间的距离\n",
    "    \n",
    "    # 1. 计算目标半径的比例，并缩放输入向量 x\n",
    "    target_radius_ratio = torch.tensor(target_radius / 6.2126)\n",
    "    \n",
    "    # 缩放 x 到目标半径，得到 geodesic_start_short\n",
    "    geodesic_start_short = gmath.mobius_scalar_mul(r=target_radius_ratio, x=x, k=torch.tensor(-1.0))\n",
    "    print('start_radius', dist0(gmath.mobius_scalar_mul(r=torch.tensor(1), x=geodesic_start_short, k=torch.tensor(-1.0))))\n",
    "    # 外层循环遍历每个距离\n",
    "    for distance in distances:\n",
    "        # 内层循环生成每个距离下的多个样本\n",
    "        for perturb_code in perturb_codes:\n",
    "            # 2. 生成随机方向\n",
    "            # random_direction = torch.randn_like(geodesic_start_short)/100\n",
    "            random_direction = perturb_code\n",
    "            \n",
    "            #print(dist0(gmath.mobius_scalar_mul(r=torch.tensor(1), x=random_direction, k=torch.tensor(-1.0))))\n",
    "            # 3. 使用 dist0 计算随机方向的双曲距离，并调整使其与 geodesic_start_short 一样长\n",
    "            r_change_direction_to = target_radius / dist0(gmath.mobius_scalar_mul(r=torch.tensor(1), x=random_direction, k=torch.tensor(-1.0)))\n",
    "            geodesic_end_short = gmath.mobius_scalar_mul(r=r_change_direction_to, x=random_direction, k=torch.tensor(-1.0))\n",
    "            # print(dist0(gmath.mobius_scalar_mul(r=torch.tensor(1), x=geodesic_end_short, k=torch.tensor(-1.0))))\n",
    "            # 4. 计算 geodesic_start_short 和 geodesic_end_short 之间的双曲距离\n",
    "            distance_x_y = gmath.dist(geodesic_start_short, geodesic_end_short, k=torch.tensor(-1.0))\n",
    "            # print('current_distance',distance_x_y)\n",
    "            # 5. 计算插值比例 t，确保 geodesic_start_short 和 feature_geo_current 之间的双曲距离等于指定的 distance\n",
    "            if distance_x_y > 0:\n",
    "                t = distance / distance_x_y  # 插值比例，确保按照双曲距离采样\n",
    "            else:\n",
    "                t = torch.tensor(1.0)  # 当距离极小时，设为1\n",
    "            \n",
    "            # 6. 沿着 geodesic_start_short 和 geodesic_end_short 插值生成 feature_geo_current\n",
    "            feature_geo_current = gmath.geodesic(t=t, x=geodesic_start_short, y=geodesic_end_short, k=torch.tensor(-1.0))\n",
    "            \n",
    "            # 7. 保存未归一化的 feature_geo_current\n",
    "            feature_geo.append(feature_geo_current)\n",
    "\n",
    "            # print('current_radius', dist0(gmath.mobius_scalar_mul(r=torch.tensor(1), x=feature_geo_current, k=torch.tensor(-1.0))))\n",
    "            # 8. 修正到目标半径，确保 feature_geo_current 的半径与 target_radius 一致\n",
    "            r_change = target_radius / dist0(gmath.mobius_scalar_mul(r=torch.tensor(1), x=feature_geo_current, k=torch.tensor(-1.0)))\n",
    "            # print('change ratio for interpolation sample',r_change)\n",
    "            feature_geo_current_target_radius = gmath.mobius_scalar_mul(r=r_change, x=feature_geo_current, k=torch.tensor(-1.0))\n",
    "            feature_geo_normalized.append(feature_geo_current_target_radius)\n",
    "            \n",
    "            # 9. 计算扰动点到起始点的双曲距离，并保存\n",
    "            dist = gmath.dist(geodesic_start_short, feature_geo_current_target_radius, k=torch.tensor(-1.0))\n",
    "            dist_to_start.append(dist)\n",
    "            \n",
    "            # 10. 保存每个 feature_geo_current 和 geodesic_start_short 的双曲距离\n",
    "            perturbation_distances.append(gmath.dist(geodesic_start_short, feature_geo_current, k=torch.tensor(-1.0)))\n",
    "            \n",
    "            # 11. 将扰动点调整到边界\n",
    "            r_change_to_boundary = 6.2126 / dist0(gmath.mobius_scalar_mul(r=torch.tensor(1), x=feature_geo_current, k=torch.tensor(-1.0)))\n",
    "            feature_geo_current_target_boundary = gmath.mobius_scalar_mul(r=r_change_to_boundary, x=feature_geo_current, k=torch.tensor(-1.0))\n",
    "            feature_geo_current_target_boundaries.append(feature_geo_current_target_boundary)\n",
    "    \n",
    "    return feature_geo, feature_geo_normalized, feature_geo_current_target_boundaries, dist_to_start, perturbation_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define animalfaces variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables\n",
    "init_image_path = './inputs/same_domain_test/animalfaces/test1.jpg'\n",
    "ref_image_path = './inputs/same_domain_test/animalfaces/test1.jpg'\n",
    "ref_image_2_path = './inputs/same_domain_test/animalfaces/test2.jpg'\n",
    "# sampling image\n",
    "random.seed(50)\n",
    "files = glob.glob(\"/data2/mhf/DXL/Lingxiao/datasets/animals/*/*.jpg\")\n",
    "sampled_imgs = random.sample(files, 5)\n",
    "\n",
    "outdir = './outputs/animalfaces_yi'\n",
    "skip_grid = False\n",
    "skip_save = True\n",
    "ddim_steps = 50\n",
    "ddim_eta = 0.0\n",
    "n_iter = 1\n",
    "C = 4\n",
    "f = 8\n",
    "n_samples = 4\n",
    "n_rows = 0\n",
    "scale = 7.5\n",
    "strength = 1.0\n",
    "config_path = './configs/stable-diffusion/v2_inference_animalfaces.yaml'\n",
    "ckpt = '/data2/mhf/DXL/Lingxiao/Codes/Paint-by-Example-test/models/Paint-by-Example/animal_faces/2024-09-11T11-30-12_v1/checkpoints/epoch=000035.ckpt'\n",
    "seed = 3408\n",
    "precision = 'autocast'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define vggfaces variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables\n",
    "init_image_path = './inputs/same_domain_test/vggfaces/40.jpg'\n",
    "ref_image_path = './inputs/same_domain_test/vggfaces/40.jpg'\n",
    "ref_image_2_path = './inputs/same_domain_test/vggfaces/18.jpg'\n",
    "outdir = './outputs/vggfaces'\n",
    "skip_grid = False\n",
    "skip_save = True\n",
    "ddim_steps = 50\n",
    "ddim_eta = 0.0\n",
    "n_iter = 1\n",
    "C = 4\n",
    "f = 8\n",
    "n_samples = 4\n",
    "n_rows = 0\n",
    "scale = 7.5\n",
    "strength = 1.0\n",
    "config_path = './configs/stable-diffusion/v2_inference_vggfaces.yaml'\n",
    "ckpt = '/data2/mhf/DXL/Lingxiao/Codes/Paint-by-Example-test/models/Paint-by-Example/vgg_faces/2024-10-02T02-12-51_v1/checkpoints/epoch=000037-ffhq.ckpt'\n",
    "seed = 3408\n",
    "precision = 'autocast'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define flowers variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables\n",
    "init_image_path = './inputs/same_domain_test/flowers/0.jpg'\n",
    "ref_image_path = './inputs/same_domain_test/flowers/0.jpg'\n",
    "ref_image_2_path = './inputs/same_domain_test/flowers/1.jpg'\n",
    "outdir = './outputs/flowers'\n",
    "skip_grid = False\n",
    "skip_save = True\n",
    "ddim_steps = 50\n",
    "ddim_eta = 0.0\n",
    "n_iter = 1\n",
    "C = 4\n",
    "f = 8\n",
    "n_samples = 4\n",
    "n_rows = 0\n",
    "scale = 7.5\n",
    "strength = 1.0\n",
    "config_path = './configs/stable-diffusion/v2_inference_flowers.yaml'\n",
    "ckpt = '/data2/mhf/DXL/Lingxiao/Codes/Paint-by-Example-test/models/Paint-by-Example/flowers/2024-10-12T05-45-07_v1/checkpoints/epoch=000298.ckpt'\n",
    "seed = 3408\n",
    "precision = 'autocast'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 3408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /data2/mhf/DXL/Lingxiao/Codes/Paint-by-Example-test/models/Paint-by-Example/animal_faces/2024-09-11T11-30-12_v1/checkpoints/epoch=000035.ckpt\n",
      "Global Step: 77724\n",
      "No module 'xformers'. Proceeding without it.\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 865.91 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data2/mhf/DXL/Lingxiao/Cache/huggingface/hub/models--openai--clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'logit_scale', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_projection.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'visual_projection.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.7.layer_norm1.bias']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use hyperbolic: True\n",
      "Loading HAE from checkpoint: /data2/mhf/DXL/Lingxiao/Codes/hyperediting/exp_out/hyper_styleGANinversion_animalfaces_512_5_30_init_v2/checkpoints/iteration_11000.pt\n",
      "Successfully loaded model!\n"
     ]
    }
   ],
   "source": [
    "seed_everything(seed)\n",
    "\n",
    "config = OmegaConf.load(f\"{config_path}\")\n",
    "model = load_model_from_config(config, f\"{ckpt}\")\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "print(\"Successfully loaded model!\")\n",
    "\n",
    "sampler = DDIMSampler(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Delta Mapper for text-guided editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data2/mhf/DXL/Lingxiao/Cache/huggingface/hub/models--openai--clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'logit_scale', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'visual_projection.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_projection.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use hyperbolic: True\n",
      "Loading HAE from checkpoint: /data2/mhf/DXL/Lingxiao/Codes/DeltaHyperEditing/exp_out/hyper_ffhq_512_5_30_v1/checkpoints/iteration_36000.pt\n",
      "Model successfully loaded!\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "sys.path.insert(0, '/data2/mhf/DXL/Lingxiao/Codes')\n",
    "from DeltaHyperEditing.models.delta_hyp_clip import hae_clip\n",
    "\n",
    "# model_path = '/data2/mhf/DXL/Lingxiao/Codes/DeltaHyperEditing/exp_out/hyper_animalfaces_512_5_30_v3/checkpoints/iteration_8000.pt'\n",
    "model_path = '/data2/mhf/DXL/Lingxiao/Codes/DeltaHyperEditing/exp_out/hyper_ffhq_512_5_30_v1/checkpoints/iteration_36000.pt'\n",
    "# model_path = '/data2/mhf/DXL/Lingxiao/Codes/DeltaHyperEditing/exp_out/hyper_flowers_512_5_30_v1/checkpoints/iteration_26000.pt'\n",
    "ckpt = torch.load(model_path, map_location='cpu')\n",
    "opts = ckpt['opts']\n",
    "del ckpt\n",
    "opts['checkpoint_path'] = model_path\n",
    "# instantialize model with checkpoints and args\n",
    "opts = Namespace(**opts)\n",
    "net = hae_clip(opts)\n",
    "net.eval()\n",
    "net.to(device)\n",
    "print('Model successfully loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(outdir, exist_ok=True)\n",
    "outpath = outdir\n",
    "\n",
    "batch_size = n_samples\n",
    "n_rows = n_rows if n_rows > 0 else batch_size\n",
    "\n",
    "sample_path = os.path.join(outpath, \"samples\")\n",
    "os.makedirs(sample_path, exist_ok=True)\n",
    "base_count = len(os.listdir(sample_path))\n",
    "grid_count = len(os.listdir(outpath)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded input image of size (175, 159) from ./inputs/same_domain_test/animalfaces/test1.jpg\n",
      "loaded input image of size (175, 159) from ./inputs/same_domain_test/animalfaces/test1.jpg\n",
      "loaded input image of size (175, 159) from ./inputs/same_domain_test/animalfaces/test1.jpg\n",
      "loaded input image of size (198, 226) from ./inputs/same_domain_test/animalfaces/test2.jpg\n",
      "loaded input image of size (134, 147) from /data2/mhf/DXL/Lingxiao/datasets/animals/n02088364/n02088364_13877.JPEG_193_107_340_241.jpg\n",
      "loaded input image of size (126, 132) from /data2/mhf/DXL/Lingxiao/datasets/animals/n02093428/n02093428_8947.JPEG_165_10_297_136.jpg\n",
      "loaded input image of size (154, 167) from /data2/mhf/DXL/Lingxiao/datasets/animals/n02109047/n02109047_6862.JPEG_17_59_184_213.jpg\n",
      "loaded input image of size (187, 193) from /data2/mhf/DXL/Lingxiao/datasets/animals/n02090379/n02090379_2046.JPEG_235_78_428_265.jpg\n",
      "loaded input image of size (127, 148) from /data2/mhf/DXL/Lingxiao/datasets/animals/n02099712/n02099712_1941.JPEG_196_46_344_173.jpg\n",
      "target t_enc is 50 steps\n"
     ]
    }
   ],
   "source": [
    "# load images\n",
    "# load init image\n",
    "assert os.path.isfile(init_image_path)\n",
    "init_image = load_img(init_image_path, [512, 512]).to(device)\n",
    "init_image_resized = load_img(init_image_path, [224, 224]).to(device)\n",
    "init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
    "init_image_resized = repeat(init_image_resized, '1 ... -> b ...', b=batch_size)\n",
    "init_latent = model.get_first_stage_encoding(\n",
    "    model.encode_first_stage(init_image))  # move to latent space\n",
    "\n",
    "# load ref image\n",
    "assert os.path.isfile(ref_image_path)\n",
    "ref_image = load_img(ref_image_path, [224, 224]).to(device)\n",
    "ref_image = repeat(ref_image, '1 ... -> b ...', b=batch_size)\n",
    "\n",
    "assert os.path.isfile(ref_image_2_path)\n",
    "ref_image_2 = load_img(ref_image_2_path, [224, 224]).to(device)\n",
    "ref_image_2 = repeat(ref_image_2, '1 ... -> b ...', b=batch_size)\n",
    "\n",
    "# load sampled images\n",
    "sampled_images = []\n",
    "for sampled_img in sampled_imgs:\n",
    "    assert os.path.isfile(sampled_img)\n",
    "    sampled_image = load_img(sampled_img, [224, 224]).to(device)\n",
    "    sampled_images.append(sampled_image)\n",
    "\n",
    "sampler.make_schedule(ddim_num_steps=ddim_steps,\n",
    "                        ddim_eta=ddim_eta, verbose=False)\n",
    "\n",
    "assert 0. <= strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
    "t_enc = int(strength * ddim_steps)\n",
    "print(f\"target t_enc is {t_enc} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit latent codes using text instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'a face'\n",
    "prompt_delta = 'a face with red hair'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake_delta_s shape: torch.Size([4, 1, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/mhf/DXL/Lingxiao/Codes/HypDiffusion/ldm/modules/encoders/hyper_nets.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  k = torch.tensor(k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature shape: torch.Size([4, 1, 512])\n",
      "reconstruct_code shape: torch.Size([4, 1, 512])\n"
     ]
    }
   ],
   "source": [
    "fake_delta_s = net.get_fake_delta_s_given_data(ref_image, prompt, prompt_delta, feature_type='hyperbolic', device=device)\n",
    "print(f\"fake_delta_s shape: {fake_delta_s.shape}\")\n",
    "logits, feature, feature_dist, feature_euc = model.get_learned_conditioning(ref_image)\n",
    "_, feature_2, feature_dist_2, feature_euc_2 = model.get_learned_conditioning(ref_image_2)\n",
    "print(f\"feature shape: {feature.shape}\")\n",
    "reconstruct_code = feature + fake_delta_s\n",
    "# reconstruct_code = feature_dist + fake_delta_s\n",
    "print(f\"reconstruct_code shape: {reconstruct_code.shape}\")\n",
    "reconstruct_feature = get_condition_given_feature(model, reconstruct_code)\n",
    "reconstruct_codes = [feature_euc, reconstruct_feature]\n",
    "# reconstruct_codes = [feature_euc, reconstruct_code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512])\n",
      "tensor([6.2126], device='cuda:0', grad_fn=<MulBackward1>)\n"
     ]
    }
   ],
   "source": [
    "_, hyp_codes = get_hyp_codes_given_feature(model, reconstruct_code)\n",
    "print(hyp_codes.shape)\n",
    "hyp_code = hyp_codes[0].unsqueeze(0)\n",
    "print(gmath.dist0(hyp_code, k=torch.tensor(-1.0)))\n",
    "rescaled_codes = []\n",
    "target_radii = [6.2126, 4, 2.5, 1, 0.5, 0]\n",
    "for i in target_radii:\n",
    "    hyp_code_rescaled = rescale(i, hyp_code)\n",
    "    hyp_code_rescaled = repeat(hyp_code_rescaled, '1 ... -> b ...', b=batch_size)\n",
    "    feature_euc = get_condition_given_hyp_codes(model, hyp_code_rescaled)\n",
    "    rescaled_codes.append(feature_euc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulate latent codes in Hyperbolic space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving latent codes from edge to center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n",
      "tensor([6.2126], device='cuda:0', grad_fn=<MulBackward1>)\n"
     ]
    }
   ],
   "source": [
    "_, hyp_code = get_hyp_codes(model, ref_image[0].unsqueeze(0))\n",
    "print(hyp_code.shape)\n",
    "print(gmath.dist0(hyp_code, k=torch.tensor(-1.0)))\n",
    "# this is used for generating figure of varying radius in our paper\n",
    "rescaled_codes = []\n",
    "target_radii = [6.2126, 4, 2.5, 1, 0.5, 0]\n",
    "for i in target_radii:\n",
    "    hyp_code_rescaled = rescale(i, hyp_code)\n",
    "    hyp_code_rescaled = repeat(hyp_code_rescaled, '1 ... -> b ...', b=batch_size)\n",
    "    feature_euc = get_condition_given_hyp_codes(model, hyp_code_rescaled)\n",
    "    rescaled_codes.append(feature_euc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate latent codes in Hyperbolic space along the geodesic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n",
      "tensor([6.2126], device='cuda:0', grad_fn=<MulBackward1>)\n",
      "tensor([6.2126], device='cuda:0', grad_fn=<MulBackward1>)\n"
     ]
    }
   ],
   "source": [
    "_, hyp_code = get_hyp_codes(model, ref_image[0].unsqueeze(0))\n",
    "_, hyp_code_2 = get_hyp_codes(model, ref_image_2[0].unsqueeze(0))\n",
    "print(hyp_code.shape)\n",
    "print(gmath.dist0(hyp_code, k=torch.tensor(-1.0)))\n",
    "print(gmath.dist0(hyp_code_2, k=torch.tensor(-1.0)))\n",
    "# this is used for generating figure of varying radius in our paper\n",
    "interpolated_codes = []\n",
    "interval = [0, 0.3, 0.4, 0.5, 0.6, 0.7, 1]\n",
    "feature_geo, feature_geo_current_target_radius, feature_geo_current_target_boundary, dist_to_start = geo_interpolate_fix_r(\n",
    "    hyp_code, hyp_code_2, interval, target_radius=6.2126)\n",
    "for i in feature_geo_current_target_boundary:\n",
    "    feature = repeat(i, '1 ... -> b ...', b=batch_size)\n",
    "    feature_euc = get_condition_given_hyp_codes(model, feature)\n",
    "    interpolated_codes.append(feature_euc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perturbation given an image embedding with certain radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/mhf/DXL/Lingxiao/Codes/HypDiffusion/ldm/modules/encoders/hyper_nets.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  k = torch.tensor(k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n",
      "tensor([6.2126], device='cuda:0', grad_fn=<MulBackward1>)\n",
      "start_radius tensor([6.2126], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(6.2126, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "current_distance tensor([11.4315], device='cuda:0', grad_fn=<MulBackward1>)\n",
      "current_radius tensor([1.0788], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "change ratio for interpolation sample tensor([5.7588], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(6.2126, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "current_distance tensor([11.5921], device='cuda:0', grad_fn=<MulBackward1>)\n",
      "current_radius tensor([0.9823], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "change ratio for interpolation sample tensor([6.3247], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(6.2126, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "current_distance tensor([11.4044], device='cuda:0', grad_fn=<MulBackward1>)\n",
      "current_radius tensor([1.1005], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "change ratio for interpolation sample tensor([5.6450], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(6.2126, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "current_distance tensor([11.5857], device='cuda:0', grad_fn=<MulBackward1>)\n",
      "current_radius tensor([0.9843], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "change ratio for interpolation sample tensor([6.3117], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(6.2126, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "current_distance tensor([11.5266], device='cuda:0', grad_fn=<MulBackward1>)\n",
      "current_radius tensor([1.0222], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "change ratio for interpolation sample tensor([6.0775], device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "interpolated_codes = []\n",
    "_, hyp_code = get_hyp_codes(model, ref_image[0].unsqueeze(0))\n",
    "_, perturb_codes = get_hyp_codes(model, sampled_images)\n",
    "print(hyp_code.shape)\n",
    "print(gmath.dist0(hyp_code, k=torch.tensor(-1.0)))\n",
    "distances = [5.7]\n",
    "num_samples = len(sampled_images)\n",
    "feature_geo, feature_geo_normalized, feature_geo_current_target_boundaries, dist_to_start, perturbation_distances = geo_perturbation(\n",
    "    hyp_code, distances, target_radius=6.2126, perturb_codes=perturb_codes, num_samples=num_samples)\n",
    "for i in feature_geo_current_target_boundaries:\n",
    "    feature = repeat(i, '1 ... -> b ...', b=batch_size)\n",
    "    feature_euc = get_condition_given_hyp_codes(model, feature)\n",
    "    interpolated_codes.append(feature_euc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([5.0000], device='cuda:0', grad_fn=<MulBackward1>),\n",
       " tensor([5.0000], device='cuda:0', grad_fn=<MulBackward1>),\n",
       " tensor([5.0003], device='cuda:0', grad_fn=<MulBackward1>),\n",
       " tensor([5.0005], device='cuda:0', grad_fn=<MulBackward1>),\n",
       " tensor([5.0010], device='cuda:0', grad_fn=<MulBackward1>),\n",
       " tensor([5.0005], device='cuda:0', grad_fn=<MulBackward1>),\n",
       " tensor([5.0003], device='cuda:0', grad_fn=<MulBackward1>),\n",
       " tensor([5.0001], device='cuda:0', grad_fn=<MulBackward1>),\n",
       " tensor([5.0003], device='cuda:0', grad_fn=<MulBackward1>),\n",
       " tensor([5.0000], device='cuda:0', grad_fn=<MulBackward1>)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturbation_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate latent codes in Euclidean space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_euc shape: torch.Size([4, 1, 1024])\n"
     ]
    }
   ],
   "source": [
    "logits, feature, feature_dist, feature_euc = model.get_learned_conditioning(ref_image)\n",
    "_, feature_2, feature_dist_2, feature_euc_2 = model.get_learned_conditioning(ref_image_2)\n",
    "print('feature_euc shape:', feature_euc.shape)\n",
    "interpolated_codes = []\n",
    "interval = [0, 0.3, 0.4, 0.5, 0.6, 0.7, 1]\n",
    "for i in interval:\n",
    "    feature = (1-i) * feature_euc + i * feature_euc_2\n",
    "    interpolated_codes.append(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (4, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:07<00:00,  7.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (4, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:07<00:00,  7.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (4, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:07<00:00,  6.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (4, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:07<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (4, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:07<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your samples are ready and waiting for you here: \n",
      "./outputs/animalfaces_yi \n",
      " \n",
      "Enjoy.\n"
     ]
    }
   ],
   "source": [
    "precision_scope = autocast if precision == \"autocast\" else nullcontext\n",
    "n_rows = n_rows if n_rows > 0 else batch_size\n",
    "with torch.no_grad():\n",
    "    with precision_scope(\"cuda\"):\n",
    "        with model.ema_scope():\n",
    "            tic = time.time()\n",
    "            all_samples = list()\n",
    "            shape = [C, 64, 64]\n",
    "            # encode (scaled latent)\n",
    "            if strength < 1.0:\n",
    "                z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
    "            # print(f\"z_enc shape: {z_enc.shape}\")\n",
    "            else:\n",
    "                z_enc = torch.randn([n_samples, 4, 64, 64], device=device)\n",
    "            # decode it\n",
    "            # for c in rescaled_codes:\n",
    "            for c in interpolated_codes:\n",
    "            # for c in reconstruct_codes:\n",
    "                samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
    "                                                    conditioning=c,\n",
    "                                                    batch_size=n_samples,\n",
    "                                                    shape=shape,\n",
    "                                                    verbose=False,\n",
    "                                                    unconditional_guidance_scale=scale,\n",
    "                                                    unconditional_conditioning=c,\n",
    "                                                    eta=ddim_eta,\n",
    "                                                    x_T=z_enc)\n",
    "                '''\n",
    "                samples_ddim = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=scale,\n",
    "                                            unconditional_conditioning=uc,)\n",
    "                                            '''\n",
    "\n",
    "                x_samples = model.decode_first_stage(samples_ddim)\n",
    "                x_samples = torch.clamp(\n",
    "                    (x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "                if not skip_save:\n",
    "                    for x_sample in x_samples:\n",
    "                        x_sample = 255. * \\\n",
    "                            rearrange(x_sample.cpu().numpy(),\n",
    "                                        'c h w -> h w c')\n",
    "                        Image.fromarray(x_sample.astype(np.uint8)).save(\n",
    "                            os.path.join(sample_path, f\"{base_count:05}.png\"))\n",
    "                        base_count += 1\n",
    "                all_samples.append(x_samples)\n",
    "\n",
    "                if not skip_grid:\n",
    "                    # additionally, save as grid\n",
    "                    grid = torch.stack(all_samples, 0)\n",
    "                    grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "                    grid = make_grid(grid, nrow=n_rows)\n",
    "\n",
    "            # to image\n",
    "            grid = 255. * \\\n",
    "                rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "            Image.fromarray(grid.astype(np.uint8)).save(\n",
    "                os.path.join(outpath, f'grid-{grid_count:04}.png'))\n",
    "            grid_count += 1\n",
    "            del grid\n",
    "\n",
    "        toc = time.time()\n",
    "\n",
    "print(f\"Your samples are ready and waiting for you here: \\n{outpath} \\n\"\n",
    "        f\" \\nEnjoy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean of the variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (20, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:32<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 512, 512])\n",
      "Data shape for DDIM sampling is (20, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:32<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 512, 512])\n",
      "Data shape for DDIM sampling is (20, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:33<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 512, 512])\n",
      "Data shape for DDIM sampling is (20, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:33<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 512, 512])\n",
      "Data shape for DDIM sampling is (20, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:33<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 512, 512])\n",
      "Data shape for DDIM sampling is (20, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:33<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 512, 512])\n",
      "Your samples are ready and waiting for you here: \n",
      "./outputs \n",
      " \n",
      "Enjoy.\n"
     ]
    }
   ],
   "source": [
    "precision_scope = autocast if precision == \"autocast\" else nullcontext\n",
    "with torch.no_grad():\n",
    "    with precision_scope(\"cuda\"):\n",
    "        with model.ema_scope():\n",
    "            tic = time.time()\n",
    "            all_samples = list()\n",
    "            shape = [C, 64, 64]\n",
    "            # encode (scaled latent)\n",
    "            if strength < 1.0:\n",
    "                z_enc = sampler.stochastic_encode(\n",
    "                    init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
    "            # print(f\"z_enc shape: {z_enc.shape}\")\n",
    "            else:\n",
    "                z_enc = torch.randn([n_samples, 4, 64, 64], device=device)\n",
    "            # decode it\n",
    "            for c in rescaled_codes:\n",
    "                samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
    "                                                 conditioning=c,\n",
    "                                                 batch_size=n_samples,\n",
    "                                                 shape=shape,\n",
    "                                                 verbose=False,\n",
    "                                                 unconditional_guidance_scale=scale,\n",
    "                                                 unconditional_conditioning=c,\n",
    "                                                 eta=ddim_eta,\n",
    "                                                 x_T=z_enc)\n",
    "                '''\n",
    "                samples_ddim = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=scale,\n",
    "                                            unconditional_conditioning=uc,)\n",
    "                                            '''\n",
    "\n",
    "                x_samples = model.decode_first_stage(samples_ddim)\n",
    "                x_samples = torch.clamp(\n",
    "                    (x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "                print(x_samples.shape)\n",
    "                x_samples_mean = x_samples.mean(0).unsqueeze(0)\n",
    "                if not skip_save:\n",
    "                    for x_sample in x_samples:\n",
    "                        x_sample = 255. * \\\n",
    "                            rearrange(x_sample.cpu().numpy(),\n",
    "                                      'c h w -> h w c')\n",
    "                        Image.fromarray(x_sample.astype(np.uint8)).save(\n",
    "                            os.path.join(sample_path, f\"{base_count:05}.png\"))\n",
    "                        base_count += 1\n",
    "                all_samples.append(x_samples_mean)\n",
    "\n",
    "                if not skip_grid:\n",
    "                    # additionally, save as grid\n",
    "                    grid = torch.stack(all_samples, 0)\n",
    "                    grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "                    grid = make_grid(grid, nrow=n_rows)\n",
    "\n",
    "            # to image\n",
    "            grid = 255. * \\\n",
    "                rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "            Image.fromarray(grid.astype(np.uint8)).save(\n",
    "                os.path.join(outpath, f'grid-{grid_count:04}.png'))\n",
    "            grid_count += 1\n",
    "            del grid\n",
    "\n",
    "        toc = time.time()\n",
    "\n",
    "print(f\"Your samples are ready and waiting for you here: \\n{outpath} \\n\"\n",
    "      f\" \\nEnjoy.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Paint-by-Example",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
